[
    {
        "text": "We can see on TV and in the news.",
        "start": 8.0,
        "duration": 2.58
    },
    {
        "text": "Watson, Who is Michael Phelps. Yes.",
        "start": 10.62,
        "duration": 2.54
    },
    {
        "text": "That artificial intelligence is no longer found only in the\nmovies.",
        "start": 13.16,
        "duration": 4.3
    },
    {
        "text": "I'm sorry Dave.",
        "start": 17.56,
        "duration": 1.6
    },
    {
        "text": "I'm afraid I can't do that.",
        "start": 19.16,
        "duration": 2.22
    },
    {
        "text": "It's beginning to enter everyday life. But can a computer make decisions requiring moral judgment?",
        "start": 21.42,
        "duration": 6.82
    },
    {
        "text": "(scream)",
        "start": 28.58,
        "duration": 2.36
    },
    {
        "text": "As AI blurs the boundary between human and machine, can technology handle the complex world we live in?",
        "start": 30.94,
        "duration": 7.66
    },
    {
        "text": "Look, I need help. I need to know more about morality.",
        "start": 38.6,
        "duration": 4.54
    },
    {
        "text": "I don't know what ethics is.",
        "start": 43.67,
        "duration": 1.83
    },
    {
        "text": "Some powerful voices are starting to question what we're creating.",
        "start": 45.5,
        "duration": 3.84
    },
    {
        "text": "I think we should be very careful about artificial intelligence.",
        "start": 49.52,
        "duration": 4.16
    },
    {
        "text": "If I were to guess...at what\nour biggest existential threat is, it's probably that.",
        "start": 53.68,
        "duration": 5.48
    },
    {
        "text": "In 2015 an open letter urging caution\nin the development of artificial intelligence",
        "start": 59.16,
        "duration": 6.56
    },
    {
        "text": "in weapons systems was signed by more than 150 experts",
        "start": 65.72,
        "duration": 4.12
    },
    {
        "text": "including Elon Musk and Stephen Hawking.",
        "start": 69.96,
        "duration": 4.04
    },
    {
        "text": "The development of artificial intelligence could spell the end of the human race.",
        "start": 74.0,
        "duration": 5.12
    },
    {
        "text": "Once humans develop artificial intelligence that would take off on it's own,",
        "start": 79.26,
        "duration": 5.68
    },
    {
        "text": "and redesign itself at an ever increasing rate.",
        "start": 85.04,
        "duration": 7.86
    },
    {
        "text": "Humans who are limited by a slow biological evolution couldn't compete, and would be superseded.",
        "start": 92.9,
        "duration": 4.18
    },
    {
        "text": "Will our machines turn against us?",
        "start": 97.62,
        "duration": 3.54
    },
    {
        "text": "With artificial intelligence we are summoning the demon.",
        "start": 101.16,
        "duration": 2.26
    },
    {
        "text": "But can morality be programmed?",
        "start": 104.52,
        "duration": 8.66
    },
    {
        "text": "Siri what is the meaning of life?",
        "start": 113.18,
        "duration": 2.84
    },
    {
        "text": "I don't believe there is a consensus on that question.",
        "start": 117.22,
        "duration": 3.32
    },
    {
        "text": "Siri what is the meaning of life?",
        "start": 121.62,
        "duration": 3.039
    },
    {
        "text": "Life, a principle or force that is considered\nto underlie a distinctive quality of animate beings.",
        "start": 126.08,
        "duration": 6.02
    },
    {
        "text": "I guess that includes me.",
        "start": 132.1,
        "duration": 1.36
    },
    {
        "text": "Yeah good luck.",
        "start": 134.04,
        "duration": 1.84
    },
    {
        "text": "Siri what is the meaning of life?",
        "start": 137.02,
        "duration": 2.94
    },
    {
        "text": "I don't believe there is a consensus on that question.",
        "start": 139.96,
        "duration": 3.92
    },
    {
        "text": "Siri what is the meaning of life?",
        "start": 143.88,
        "duration": 2.78
    },
    {
        "text": "It's nothing Nietzsche couldn't teach ya.",
        "start": 146.78,
        "duration": 2.76
    },
    {
        "text": "Very funny. \nOne more. Siri what is the meaning of life?",
        "start": 150.14,
        "duration": 4.8
    },
    {
        "text": "I don't believe there is a consensus on that question.",
        "start": 155.18,
        "duration": 4.66
    },
    {
        "text": "Well it's hard to imagine our life without computers.",
        "start": 159.84,
        "duration": 3.28
    },
    {
        "text": "And sometimes we wish we were... \nI'm sorry.",
        "start": 163.12,
        "duration": 2.52
    },
    {
        "text": "Sorry, she's not completely human yet I guess.",
        "start": 166.2,
        "duration": 4.62
    },
    {
        "text": "It's hard to imagine life without these things that have sneaked into our life",
        "start": 170.82,
        "duration": 5.089
    },
    {
        "text": "like siri and MS's Cortana I believe. I'm a Mac person so I don't know about Cortana.",
        "start": 175.909,
        "duration": 5.591
    },
    {
        "text": "Computer technology has so quickly become seamlessly interwoven into everyday life.",
        "start": 181.66,
        "duration": 6.74
    },
    {
        "text": "But have we had the chance to really think about the consequences? Do we even know how?",
        "start": 188.4,
        "duration": 3.94
    },
    {
        "text": "What kind of ethical guidelines do we need for the development and use of this technology?",
        "start": 192.46,
        "duration": 5.78
    },
    {
        "text": "Who's responsible when a driverless car gets into an accident which seems inevitable.",
        "start": 198.38,
        "duration": 4.8
    },
    {
        "text": "Autonomous weapons systems\nare being developed by the U.S., the U.K.,",
        "start": 204.66,
        "duration": 3.28
    },
    {
        "text": "England, Russia...autonomous weapons and China.",
        "start": 208.0,
        "duration": 3.26
    },
    {
        "text": "And I've just learned at least a few dozen",
        "start": 211.26,
        "duration": 3.039
    },
    {
        "text": "other countries that are already developing\nautonomous weapons systems that make their own decisions.",
        "start": 214.299,
        "duration": 6.201
    },
    {
        "text": "Should a machine be given the\nautonomy to make life and death decisions?",
        "start": 220.5,
        "duration": 2.94
    },
    {
        "text": "Can morality be coded? How close are we to that",
        "start": 223.44,
        "duration": 5.2
    },
    {
        "text": "wonderful sympathetic character Commander\nData that was created in the mind of",
        "start": 228.65,
        "duration": 5.086
    },
    {
        "text": "Gene Roddenberry for Star Trek The Next Generation\n30 years ago.",
        "start": 233.74,
        "duration": 3.9
    },
    {
        "text": "So to understand these questions or get a\nway of thinking about them.",
        "start": 237.8,
        "duration": 3.46
    },
    {
        "text": "Our first guest is a scientist, a best selling\nauthor, entrepreneur and professor of psychology at NYU.",
        "start": 241.26,
        "duration": 6.82
    },
    {
        "text": "He's also the CEO and co-founder of\nthe recently formed Geometric Intelligence.",
        "start": 248.08,
        "duration": 4.52
    },
    {
        "text": "His research on language computation, artificial intelligence",
        "start": 252.609,
        "duration": 3.131
    },
    {
        "text": "and cognitive development has\nbeen published in leading journals such as",
        "start": 255.74,
        "duration": 3.05
    },
    {
        "text": "Science and Nature and several others.",
        "start": 258.79,
        "duration": 3.95
    },
    {
        "text": "He's a frequent contributor to The New Yorker and",
        "start": 262.74,
        "duration": 1.34
    },
    {
        "text": "The New York Times. Please welcome Gary Marcus.",
        "start": 264.08,
        "duration": 4.48
    },
    {
        "text": "Next up is a senior researcher at Microsoft.",
        "start": 273.62,
        "duration": 2.28
    },
    {
        "text": "Prior to joining Microsoft he was a senior\nscientist at Yahoo research. His primary research",
        "start": 275.9,
        "duration": 4.29
    },
    {
        "text": "interest is data mining, web search and evaluation\nof machine learning.",
        "start": 280.19,
        "duration": 4.405
    },
    {
        "text": "His work in the ethics of online systems has been presented at several conferences.",
        "start": 284.6,
        "duration": 4.66
    },
    {
        "text": "He's the organizer of a 2016 workshop on ethics\nof online experimentation.",
        "start": 289.26,
        "duration": 0.5
    },
    {
        "text": "Please welcome Fernando Diaz.",
        "start": 289.76,
        "duration": 6.66
    },
    {
        "text": "And next we have the director of the human robot interaction laboratory at Tufts University.",
        "start": 301.22,
        "duration": 4.62
    },
    {
        "text": "He's also a program manager of the new Center",
        "start": 305.98,
        "duration": 1.8
    },
    {
        "text": "for Applied brain and cognitive sciences joint\nprogram with the U.S. Army.",
        "start": 307.78,
        "duration": 5.18
    },
    {
        "text": "In addition to studying robot behavior he\nworks in the field of artificial life,",
        "start": 312.96,
        "duration": 3.2
    },
    {
        "text": "artificial intelligence, cognitive science and philosophy.\nPlease welcome Mathias Schuetz.",
        "start": 316.16,
        "duration": 6.64
    },
    {
        "text": "Our next guest is a consultant\nethicist and scholar at Yale University's Interdisciplinary Center for Bioethics.",
        "start": 326.54,
        "duration": 6.174
    },
    {
        "text": "He's also a senior adviser to the Hastings Center.",
        "start": 332.72,
        "duration": 2.76
    },
    {
        "text": "His latest book is A Dangerous Master: How to Keep Technology from Slipping Beyond our Control.",
        "start": 335.74,
        "duration": 5.44
    },
    {
        "text": "He also coauthored Moral Machines teaching robots right from wrong.",
        "start": 341.18,
        "duration": 3.52
    },
    {
        "text": "Please welcome Wendell Wallach.",
        "start": 344.84,
        "duration": 2.84
    },
    {
        "text": "And our final participant is the permanent professor \nand head of the Department of Law",
        "start": 352.42,
        "duration": 4.46
    },
    {
        "text": "at the United States Air Force Academy. She's\nboth an attorney and a rocket scientist with",
        "start": 356.88,
        "duration": 5.46
    },
    {
        "text": "a degree an astronomical engine... astronautical\nengineering.",
        "start": 362.34,
        "duration": 4.12
    },
    {
        "text": "She recently studied and published on the\noverlap of autonomy, national security, and ethics",
        "start": 366.46,
        "duration": 5.7
    },
    {
        "text": "at National Defense University. Please\nwelcome Colonel Linell Letendre.",
        "start": 372.16,
        "duration": 7.16
    },
    {
        "text": "Now more than half a century before Stephen Hawkings and Elon Musk",
        "start": 383.54,
        "duration": 3.842
    },
    {
        "text": "felt compelled to warn the world of artificial intelligence",
        "start": 387.382,
        "duration": 2.678
    },
    {
        "text": "back in 1942, before the term was even coined,",
        "start": 390.06,
        "duration": 3.04
    },
    {
        "text": "the science fiction writer Isaac Asimov wrote the three laws of robotics a moral code to",
        "start": 393.1,
        "duration": 8.56
    },
    {
        "text": "keep our machines in check. And the three\nlaws of robotics are: A robot may not injure",
        "start": 401.66,
        "duration": 6.67
    },
    {
        "text": "a human being or through inaction allow a\nhuman being to come to harm.",
        "start": 408.33,
        "duration": 5.15
    },
    {
        "text": "The second law, a robot must obey orders given by human beings except for such",
        "start": 413.48,
        "duration": 5.26
    },
    {
        "text": "orders would conflict with the first law.",
        "start": 418.84,
        "duration": 3.4
    },
    {
        "text": "And the third, a robot must\nprotect its own existence as long as such",
        "start": 422.38,
        "duration": 4.939
    },
    {
        "text": "protection does not conflict with the first\nand the second law.",
        "start": 427.319,
        "duration": 3.841
    },
    {
        "text": "That sounds logical.",
        "start": 431.16,
        "duration": 1.44
    },
    {
        "text": "Do these three laws provide a basis to work from to develop moral robots.",
        "start": 432.6,
        "duration": 1.6
    },
    {
        "text": "Marcus what do you think?",
        "start": 434.2,
        "duration": 6.28
    },
    {
        "text": "I think they make for good science fiction. There are lots of plots that you can turn around",
        "start": 440.86,
        "duration": 5.24
    },
    {
        "text": "having these kinds of laws.",
        "start": 446.3,
        "duration": 2.2
    },
    {
        "text": "But the first problem, if you've ever programed anything is\na concept like harm is really hard to program",
        "start": 448.5,
        "duration": 4.979
    },
    {
        "text": "into a machine. It's one thing to program\nin geometry or compound interest or something",
        "start": 453.479,
        "duration": 4.101
    },
    {
        "text": "like that where we have precise necessary\nand sufficient conditions. Nobody has any",
        "start": 457.58,
        "duration": 4.049
    },
    {
        "text": "idea how to, in a generalized way get a machine\nto recognize something like harm or justice.",
        "start": 461.629,
        "duration": 4.741
    },
    {
        "text": "So there is a very serious programming problem.\nThen there are a couple of other problems too.",
        "start": 466.37,
        "duration": 5.55
    },
    {
        "text": "One is that not everybody would agree\nthat the robot should never allow a human",
        "start": 471.92,
        "duration": 2.399
    },
    {
        "text": "to come to harm and would it what if for example\nwe're talking about a terrorist or a sniper",
        "start": 474.319,
        "duration": 4.431
    },
    {
        "text": "or something like that. I mean some people\nnot everybody but some people might actually",
        "start": 478.75,
        "duration": 3.36
    },
    {
        "text": "want to allow that into what they would let\nrobots do.",
        "start": 482.11,
        "duration": 3.7
    },
    {
        "text": "And then the third issue. You really think\nthrough the third one of those laws is it",
        "start": 485.81,
        "duration": 4.419
    },
    {
        "text": "sets up robots to be second class citizens\nand ultimately to be slaves. And right now",
        "start": 490.229,
        "duration": 4.4
    },
    {
        "text": "that might seem ok because robots don't seem\nvery clever but as they get smarter and smarter",
        "start": 494.629,
        "duration": 3.611
    },
    {
        "text": "they might resent that or it might not feel\nlike appropriate thing to do.",
        "start": 498.24,
        "duration": 3.34
    },
    {
        "text": "You mean those laws might not be fair to robots?",
        "start": 501.66,
        "duration": 1.58
    },
    {
        "text": "They might not be fair to robots. Exactly what I'm saying.",
        "start": 503.24,
        "duration": 4.22
    },
    {
        "text": "But the problem is not just with the machines",
        "start": 507.46,
        "duration": 2.35
    },
    {
        "text": "but our ethical code itself surely. Do we\nknow what fair is? That is if we agree we",
        "start": 509.81,
        "duration": 4.48
    },
    {
        "text": "should be fair to robots.",
        "start": 514.29,
        "duration": 1.63
    },
    {
        "text": "That's part of the problem is we don't know",
        "start": 515.92,
        "duration": 1.809
    },
    {
        "text": "what code we should program in. So Asimov's\nLaws are a nice starting point at least for",
        "start": 517.729,
        "duration": 4.531
    },
    {
        "text": "a novel. But for example imagine that we programmed\nin our laws from the 17th century and we would",
        "start": 522.26,
        "duration": 5.68
    },
    {
        "text": "have thought slavery was okay. So maybe you\ndon't want to program in the fixed laws that",
        "start": 527.94,
        "duration": 4.23
    },
    {
        "text": "we have right now to shackle the robots forever.\nWe don't want to burn them into the ROM chips",
        "start": 532.17,
        "duration": 4.099
    },
    {
        "text": "of the robots but we also don't know how we\nwant the morals to grow over time and so it's",
        "start": 536.269,
        "duration": 6.331
    },
    {
        "text": "a very complicated issue.",
        "start": 542.6,
        "duration": 1.54
    },
    {
        "text": "Sounds like it. Wendell Wallach, why is developing",
        "start": 544.14,
        "duration": 2.1
    },
    {
        "text": "a moral code for humans such a challenge?",
        "start": 546.24,
        "duration": 3.1
    },
    {
        "text": "I'm going to come back to that but I'm going",
        "start": 549.34,
        "duration": 1.69
    },
    {
        "text": "to first start with this question about Asimov's\nlaw. It's important to note that he wrote",
        "start": 551.03,
        "duration": 5.179
    },
    {
        "text": "more than 80 stories about robots. Most of\nthem around these laws. And if you list if",
        "start": 556.209,
        "duration": 5.461
    },
    {
        "text": "you read the stories you realize that in nearly\nevery one of them the robots cannot function",
        "start": 561.67,
        "duration": 5.23
    },
    {
        "text": "properly under these three pretty straightforward\nlaws. So consider a situation where you have",
        "start": 566.9,
        "duration": 6.74
    },
    {
        "text": "commands from two different people that are\ncounter to each other. So in situations like",
        "start": 573.64,
        "duration": 6.389
    },
    {
        "text": "that Asimov largely showed us that a simple\nrule based morality does not work. So that's",
        "start": 580.029,
        "duration": 5.861
    },
    {
        "text": "a partial answer to your to your question\nabout why morality is so difficult to program in.",
        "start": 585.89,
        "duration": 6.85
    },
    {
        "text": "Well the the degree to which these robots\nof all kind of simple and very complex are",
        "start": 592.74,
        "duration": 10.81
    },
    {
        "text": "difficult to think about is that they're surrounding\nus all over the place. Now we see them everywhere.",
        "start": 603.55,
        "duration": 4.46
    },
    {
        "text": "We see them in our health care system, in\nour cars and our stock market thinking.",
        "start": 608.01,
        "duration": 4.61
    },
    {
        "text": "Which is hard to think about it but the stock market\napparently does think.",
        "start": 612.62,
        "duration": 3.16
    },
    {
        "text": "Revolutionizing the battlefield and closer to home in our laptops.",
        "start": 615.78,
        "duration": 3.88
    },
    {
        "text": "When did talking to software, speech recognition software get started?",
        "start": 619.66,
        "duration": 5.84
    },
    {
        "text": "Did it start with Siri?",
        "start": 625.5,
        "duration": 2.3
    },
    {
        "text": "Well, I mean speech recognition software has been around for a long time",
        "start": 627.98,
        "duration": 3.02
    },
    {
        "text": "and even before there\nwas good speech recognition software there",
        "start": 631.0,
        "duration": 2.72
    },
    {
        "text": "are things like Eliza. I think we're going\nto see a clip of a Eliza. Eliza was a system",
        "start": 633.72,
        "duration": 4.05
    },
    {
        "text": "that you could talk to by teletype which is\nsort of like a forerunner of text message.",
        "start": 637.77,
        "duration": 3.98
    },
    {
        "text": "And how long ago was?",
        "start": 641.75,
        "duration": 1.59
    },
    {
        "text": "That was in the 1960s almost 50 years ago",
        "start": 643.34,
        "duration": 1.869
    },
    {
        "text": "and you had this conversation with Siri where\nif you asked her just once what was the meaning",
        "start": 645.209,
        "duration": 4.431
    },
    {
        "text": "of life, It would seem like a plausible wisecracker\nanswer but you got the same answer three times",
        "start": 649.64,
        "duration": 4.999
    },
    {
        "text": "in a row in your live demo. And when you get\nthe same answer there are three out of five",
        "start": 654.639,
        "duration": 4.45
    },
    {
        "text": "times you realize that there's not that much\nthere there and that was the problem with",
        "start": 659.089,
        "duration": 3.121
    },
    {
        "text": "Eliza is that it would repeat itself.",
        "start": 662.21,
        "duration": 1.89
    },
    {
        "text": "So let's take a look at this 1966 clip of",
        "start": 664.26,
        "duration": 3.0
    },
    {
        "text": "somebody trying to get some advice from Eliza\nif we have it.",
        "start": 667.269,
        "duration": 3.44
    },
    {
        "text": "Didn't you have some psychiatric problem.\nType it in and see what happens.",
        "start": 670.709,
        "duration": 4.391
    },
    {
        "text": "I'm depressed.",
        "start": 675.18,
        "duration": 1.64
    },
    {
        "text": "My daughter left home, ok.",
        "start": 683.56,
        "duration": 2.44
    },
    {
        "text": "Please go on. Well I certainly don't want to get into trouble.",
        "start": 694.06,
        "duration": 4.58
    },
    {
        "text": "What would it mean to you if she gets into trouble?",
        "start": 706.56,
        "duration": 2.032
    },
    {
        "text": "Well wouldn't you worry? You're just a dumb machine anyway. I'm going to tell you so in your own language.",
        "start": 708.6,
        "duration": 4.72
    },
    {
        "text": "Do computers frighten you?",
        "start": 723.94,
        "duration": 1.313
    },
    {
        "text": "You know. That's really incredible because its as if it really understood what I was saying.",
        "start": 725.26,
        "duration": 4.06
    },
    {
        "text": "Very smart but just rote answers.",
        "start": 731.32,
        "duration": 3.3
    },
    {
        "text": "That's kind of what we're getting out of Siri\ntoo is a lot of rote answers so your meaning",
        "start": 735.14,
        "duration": 5.02
    },
    {
        "text": "of life question is just getting one wrote answer after another. Please go on is a rote answer that Eliza gave.",
        "start": 740.16,
        "duration": 5.96
    },
    {
        "text": "So you know Ray Kurtzweil\ntalks about the exponential growth in AI how",
        "start": 746.22,
        "duration": 3.2
    },
    {
        "text": "it's getting faster and faster. But if you\nlook at something like this Siri basically",
        "start": 749.42,
        "duration": 3.859
    },
    {
        "text": "works in the same way as Eliza. We haven't\nhad that exponential progress.",
        "start": 753.279,
        "duration": 2.891
    },
    {
        "text": "She doesn't understand anything to speak of.",
        "start": 756.17,
        "duration": 2.63
    },
    {
        "text": "She understands some things about sports scores",
        "start": 758.8,
        "duration": 1.3
    },
    {
        "text": "and the weather but she doesn't have a broad\nunderstanding of human dynamics.",
        "start": 760.1,
        "duration": 3.83
    },
    {
        "text": "So Fernando about how far have we come in\ngeneral since Eliza. That was 1966 where other",
        "start": 763.93,
        "duration": 5.87
    },
    {
        "text": "kinds of powers do we have?",
        "start": 769.8,
        "duration": 1.3
    },
    {
        "text": "Well I mean I think in terms of speech recognition",
        "start": 771.1,
        "duration": 1.989
    },
    {
        "text": "for example we have gone a lot farther. But\nI think as it was said like the back end of",
        "start": 773.089,
        "duration": 6.711
    },
    {
        "text": "Siri or Cortana or in a lot of these systems\nare pretty pretty simple. They're pinging",
        "start": 779.8,
        "duration": 5.71
    },
    {
        "text": "back ends like Bing or like Google to retrieve\nan answer and or their canned answers that",
        "start": 785.51,
        "duration": 5.73
    },
    {
        "text": "you're going to notice if you repeat the question\nover and over again. But I think I think one",
        "start": 791.24,
        "duration": 4.49
    },
    {
        "text": "of the interesting things about Eliza specifically\nis that it shows like one of these domains",
        "start": 795.73,
        "duration": 5.37
    },
    {
        "text": "in which AI or machine learning is being used\nthat is a very very personal interaction that",
        "start": 801.1,
        "duration": 7.38
    },
    {
        "text": "a human is having with the machine.",
        "start": 808.48,
        "duration": 2.96
    },
    {
        "text": "People are discussing their mental health issues",
        "start": 811.44,
        "duration": 2.28
    },
    {
        "text": "with the machine. And so as a result a lot\nof the decisions that we're making as engineers",
        "start": 813.72,
        "duration": 5.179
    },
    {
        "text": "are designing these things will have very\nprofound impacts perhaps on the individuals",
        "start": 818.899,
        "duration": 4.761
    },
    {
        "text": "interacting those with those machines.",
        "start": 823.66,
        "duration": 1.94
    },
    {
        "text": "So things like neural networks where you hear",
        "start": 826.48,
        "duration": 4.18
    },
    {
        "text": "terms like that nowadays but are you suggesting\nthat we're still in a quantitative not a qualitative",
        "start": 830.66,
        "duration": 7.84
    },
    {
        "text": "world of difference from the ancient Eliza?",
        "start": 839.48,
        "duration": 3.6
    },
    {
        "text": "Yeah I mean I think I think it was said I",
        "start": 843.62,
        "duration": 3.04
    },
    {
        "text": "think a lot of the technology is very similar\nto Eliza as well. I mean in terms of the response",
        "start": 846.67,
        "duration": 6.96
    },
    {
        "text": "generation, the recognition is new. What worries\nme a little bit more is that the I guess the",
        "start": 853.63,
        "duration": 5.329
    },
    {
        "text": "moral understanding about how to develop these\nsystems has not really progressed very much",
        "start": 858.959,
        "duration": 3.721
    },
    {
        "text": "at all as much as say neural networks et cetera.",
        "start": 862.68,
        "duration": 2.9
    },
    {
        "text": "Ah, So they may be dangerous in ways that we are just beginning to discover.",
        "start": 865.64,
        "duration": 2.34
    },
    {
        "text": "That's right.",
        "start": 868.14,
        "duration": 0.999
    },
    {
        "text": "Mathias how how much closer than Eliza are",
        "start": 869.139,
        "duration": 5.491
    },
    {
        "text": "we now to Mr. Data. Commander Data?",
        "start": 874.63,
        "duration": 2.95
    },
    {
        "text": "Well we are certainly very far from Mr. Data.",
        "start": 877.58,
        "duration": 2.74
    },
    {
        "text": "There's no doubt about it. And people who\nclaim otherwise are just wrong.",
        "start": 880.32,
        "duration": 3.96
    },
    {
        "text": "But the big challenge today that we haven't really solved is genuine natural language understanding.",
        "start": 884.28,
        "duration": 6.39
    },
    {
        "text": "So Eliza for example would do a very surface,\nvery superficial analysis of what was typed",
        "start": 890.67,
        "duration": 6.44
    },
    {
        "text": "in and typically turn a statement around into\na question. And people kept going because",
        "start": 897.11,
        "duration": 6.289
    },
    {
        "text": "they thought there was a genuine question.\nBut there wasn't a genuine question because",
        "start": 903.4,
        "duration": 2.88
    },
    {
        "text": "Eliza had no understanding of what Eliza was asking.",
        "start": 906.28,
        "duration": 3.76
    },
    {
        "text": "So we want to change that especially",
        "start": 910.4,
        "duration": 2.7
    },
    {
        "text": "if you're moving towards interacting with\nautonomous systems like robots and we want to",
        "start": 913.1,
        "duration": 4.58
    },
    {
        "text": "give them instructions to do things in the\nworld, they need to have an understanding",
        "start": 917.68,
        "duration": 3.99
    },
    {
        "text": "of what we instructed them to do. So we're\npushing that. The baby steps.",
        "start": 921.67,
        "duration": 4.71
    },
    {
        "text": "We saw Watson for example that could answer questions and understand the meaning of questions.",
        "start": 926.38,
        "duration": 5.8
    },
    {
        "text": "Watson who beats some grandmasters at chess\nI believe or was it that was the game show?",
        "start": 932.18,
        "duration": 6.62
    },
    {
        "text": "That was deep blue. Watson won at Jeopardy.",
        "start": 938.8,
        "duration": 2.08
    },
    {
        "text": "Right. Well since Watson who won at Jeopardy and Deep Blue who won at chess",
        "start": 941.04,
        "duration": 3.26
    },
    {
        "text": "we now have an example of the latest computer technology with alpha go.",
        "start": 944.3,
        "duration": 5.4
    },
    {
        "text": "And I believe we have a video\nthat can explain what Alpha go manages to do.",
        "start": 950.34,
        "duration": 6.68
    },
    {
        "text": "This is the Go game and the Go game is\nsaid to have an enormous number of possible answers",
        "start": 957.02,
        "duration": 8.063
    },
    {
        "text": "but they're infinitely more - this\nis just a few of them - then chess has.",
        "start": 965.083,
        "duration": 6.297
    },
    {
        "text": "Chess has just a few answers but Go they really don't know how many answers",
        "start": 971.38,
        "duration": 6.4
    },
    {
        "text": "total combinations there are with which you can win Go.",
        "start": 977.78,
        "duration": 3.08
    },
    {
        "text": "We've seen ones estimated at more molecules than there are in the universe and that seems doubtful.",
        "start": 982.36,
        "duration": 3.02
    },
    {
        "text": "It's extraordinarily complex. And yet Alpha Go is a computer that  beat the world champion just very recently.",
        "start": 985.6,
        "duration": 9.28
    },
    {
        "text": "So what's so impressive about Alpha go?",
        "start": 994.89,
        "duration": 3.41
    },
    {
        "text": "Go has been a challenging game because of",
        "start": 998.3,
        "duration": 1.81
    },
    {
        "text": "what you mentioned. The branching factor that\nthat is at any point when you can make a move",
        "start": 1000.11,
        "duration": 3.669
    },
    {
        "text": "there's lots and lots of choices that you\nhave to make a subsequent move. And traditional",
        "start": 1003.779,
        "duration": 4.12
    },
    {
        "text": "techniques in AI that sort of look at the\nsubsequent, you know, moves and then at the",
        "start": 1007.899,
        "duration": 4.841
    },
    {
        "text": "move against that move and the move for that\nmove did not really work. The choices were",
        "start": 1012.74,
        "duration": 5.8
    },
    {
        "text": "too large and as you can see in this graphic\nthe tree that is being built by looking at",
        "start": 1018.54,
        "duration": 4.44
    },
    {
        "text": "all the combinations is too large we used\na very different technique to solve the problem.",
        "start": 1022.98,
        "duration": 5.05
    },
    {
        "text": "Right Fernando you find it impressive?",
        "start": 1028.03,
        "duration": 2.81
    },
    {
        "text": "I mean I think a lot of the techniques that",
        "start": 1030.84,
        "duration": 1.41
    },
    {
        "text": "were used for Alpha Go have been around since\nthe 80s frankly with some minor tweaks and",
        "start": 1032.25,
        "duration": 6.53
    },
    {
        "text": "what's advanced has been frankly the hardware and the data and given that this thing, that",
        "start": 1038.78,
        "duration": 4.59
    },
    {
        "text": "the technology, the hardware has advanced,\nit's allowed us to implement these techniques",
        "start": 1043.37,
        "duration": 4.02
    },
    {
        "text": "and developed systems like this. But am I\nsurprised that a machine can beat a human at go?",
        "start": 1047.39,
        "duration": 4.81
    },
    {
        "text": "Well no. As you said the state space\nof go, the number of combinations of boards is huge.",
        "start": 1052.2,
        "duration": 5.42
    },
    {
        "text": "And that's what makes it hard for humans but machines are better at counting than humans.",
        "start": 1057.62,
        "duration": 5.4
    },
    {
        "text": "So quantitative not necessarily qualitative.",
        "start": 1063.02,
        "duration": 2.34
    },
    {
        "text": "Gary, you think is also not so impressive?",
        "start": 1065.36,
        "duration": 2.44
    },
    {
        "text": "Well I mean it's impressive that they did",
        "start": 1067.8,
        "duration": 1.76
    },
    {
        "text": "this several years before people thought that\nit would happen. But you have to remember",
        "start": 1069.56,
        "duration": 4.35
    },
    {
        "text": "if you're thinking about like are the robots\ngoing to take over now, that in Go you're",
        "start": 1073.91,
        "duration": 4.53
    },
    {
        "text": "relying on a very fixed world. The rules are\nalways the same. You can play against yourself",
        "start": 1078.44,
        "duration": 4.25
    },
    {
        "text": "hundreds of millions of times",
        "start": 1082.69,
        "duration": 1.922
    },
    {
        "text": "and you can simulate things you get a lot of data",
        "start": 1084.612,
        "duration": 3.248
    },
    {
        "text": "and in the real world things are constantly\nchanging. And they're constantly changing",
        "start": 1087.86,
        "duration": 3.55
    },
    {
        "text": "and we can't simulate them perfectly. So there was this DARPA competition last year where",
        "start": 1091.41,
        "duration": 4.07
    },
    {
        "text": "robots had to do things like open doors and\ndrive cars and things like that. And there",
        "start": 1095.48,
        "duration": 4.57
    },
    {
        "text": "was a YouTube video which you should all go home and look at about bloopers from this.",
        "start": 1100.05,
        "duration": 3.85
    },
    {
        "text": "And so the robots were falling over and you\nknow someone in some ragtime music or something like that.",
        "start": 1103.9,
        "duration": 5.76
    },
    {
        "text": "And it's hilarious and the important thing to remember when you watch this video",
        "start": 1109.98,
        "duration": 3.36
    },
    {
        "text": "relative to Alpha Go is that everything that was done",
        "start": 1113.34,
        "duration": 2.31
    },
    {
        "text": "in this video was actually done in simulation\nfirst. So there were robots in simulation",
        "start": 1115.65,
        "duration": 4.87
    },
    {
        "text": "were able to open the doors perfectly and\nthen the real world, you started to have things",
        "start": 1120.52,
        "duration": 3.71
    },
    {
        "text": "like friction and you know a little bit, well\nI guess gravity was already factored in but",
        "start": 1124.23,
        "duration": 4.13
    },
    {
        "text": "you had friction and wind and things like\nthat and then suddenly because things weren't",
        "start": 1128.36,
        "duration": 3.58
    },
    {
        "text": "exactly the same as in the simulation it didn't\nwork that well anymore. The techniques in",
        "start": 1131.94,
        "duration": 4.0
    },
    {
        "text": "Alpha Go at least at this stage are not I\nthink robust to going from a simulation to",
        "start": 1135.94,
        "duration": 4.86
    },
    {
        "text": "a real world. They're relying on the fact\nthat you can get a lot of data in simulation and so that's",
        "start": 1140.8,
        "duration": 4.45
    },
    {
        "text": "a limit at least for now for how this system\ngoes it might be a component in a larger system some day.",
        "start": 1145.25,
        "duration": 5.307
    },
    {
        "text": "But it's not like tomorrow we're going to see robots. Well we're actually going to see a robot today.",
        "start": 1150.557,
        "duration": 4.877
    },
    {
        "text": "the robot that we're going  to see today is not going to be able to you know take over the world.",
        "start": 1155.434,
        "duration": 4.406
    },
    {
        "text": "It's nothing like that level cognition just yet.",
        "start": 1159.84,
        "duration": 2.94
    },
    {
        "text": "And there's some dangers we're learning as well",
        "start": 1162.96,
        "duration": 2.78
    },
    {
        "text": "There was a case recently fairly recently with something called TayTweets It's a chatbot",
        "start": 1165.74,
        "duration": 3.52
    },
    {
        "text": "that I didn't fully understand. Colonel\nLunel I see you smiling. Tell us. Tell us",
        "start": 1171.94,
        "duration": 5.18
    },
    {
        "text": "a little bit about what TayTweets(?) was and\nwhy it had some parents worried when it got",
        "start": 1177.13,
        "duration": 4.85
    },
    {
        "text": "to turning into something of a Nazi.\nWell I know we have a Microsoft expert here",
        "start": 1181.98,
        "duration": 3.97
    },
    {
        "text": "as well that might be able to explain what\nhappened with the TayTweets.",
        "start": 1185.95,
        "duration": 4.14
    },
    {
        "text": "I'll explain from my parental viewpoint what\nI thought Fernando would you like to explain",
        "start": 1190.09,
        "duration": 5.15
    },
    {
        "text": "from a microsoft perspective.\nI mean to actually, Tay's a super interesting",
        "start": 1195.24,
        "duration": 3.55
    },
    {
        "text": "case. For me it highlights I think really\nthe idea was....",
        "start": 1198.79,
        "duration": 3.74
    },
    {
        "text": "There might be members of the audience who\ndon't really know what Tay was. Lets just",
        "start": 1202.53,
        "duration": 2.47
    },
    {
        "text": "fill in briefly. Tay is something that was\nput out in Twitter. It was created by Microsoft",
        "start": 1205.0,
        "duration": 3.75
    },
    {
        "text": "and it was sort of like Eliza and you were\nsupposed to interact with it by sending tweets",
        "start": 1208.75,
        "duration": 4.12
    },
    {
        "text": "to it and and Microsoft has something called\nshowize(?) in China that is worked on somewhat",
        "start": 1212.87,
        "duration": 6.03
    },
    {
        "text": "similar principles and lots of people love\nit and use it every day. They released it",
        "start": 1218.9,
        "duration": 3.31
    },
    {
        "text": "into the wild in the United States and a lot\nof people that I would characterize as Donald",
        "start": 1222.21,
        "duration": 3.92
    },
    {
        "text": "Trump supporters had their way with Tay.\nHow many bets were there were just won about",
        "start": 1226.13,
        "duration": 11.25
    },
    {
        "text": "how soon that would come up.\nSo yesterday I brought them up after 45 minutes",
        "start": 1237.38,
        "duration": 3.84
    },
    {
        "text": "and today it was only 22 minutes or so with\nthese Donald Trump supporters. You know got",
        "start": 1241.22,
        "duration": 6.7
    },
    {
        "text": "Tay to say some things that we won't repeat\nhere but we're not pleasant. That's the background.",
        "start": 1247.92,
        "duration": 5.02
    },
    {
        "text": "But as I understand it within 24 hours the\nTay tweet was sending even some very young",
        "start": 1252.94,
        "duration": 7.2
    },
    {
        "text": "people who were talking to it. Pro Nazi propaganda\nand talking about it. Sure sure and yeah.",
        "start": 1260.14,
        "duration": 10.61
    },
    {
        "text": "But it's not so.\nI mean really what happened was was there",
        "start": 1270.75,
        "duration": 2.75
    },
    {
        "text": "was there was sort of concerted effort by\na group of people to try to manipulate the",
        "start": 1273.5,
        "duration": 4.1
    },
    {
        "text": "learning that Tay was having and the types\nof responses that she would have.",
        "start": 1277.6,
        "duration": 5.74
    },
    {
        "text": "Now I left that out and it's really important\nso like Eliza was fixed. It didn't really",
        "start": 1283.34,
        "duration": 5.55
    },
    {
        "text": "learn anything new. Somebody wrote a bunch\nof rules in advance. What's exciting about",
        "start": 1288.89,
        "duration": 3.93
    },
    {
        "text": "Tay despite the failure of the initial experiment\nis it was trying to learn about the world",
        "start": 1292.82,
        "duration": 4.45
    },
    {
        "text": "around it. Learn for example slang. Learn\nto talk with new language that wasn't all",
        "start": 1297.27,
        "duration": 4.09
    },
    {
        "text": "preprogrammed in but that was also its vulnerability.\nThat's",
        "start": 1301.36,
        "duration": 2.84
    },
    {
        "text": "right. And I guess for me the interesting,\none of the interesting lessons from Tae was",
        "start": 1304.2,
        "duration": 5.55
    },
    {
        "text": "that you know we as humans sometimes will\nbehave manipulate or behave against the best",
        "start": 1309.75,
        "duration": 5.95
    },
    {
        "text": "wishes of a of an artificial intelligence\nagent. And then what does it say about us",
        "start": 1315.7,
        "duration": 5.86
    },
    {
        "text": "that we're that we're we try to manipulate\nthis this agent that's supposed to be intelligence",
        "start": 1321.56,
        "duration": 4.06
    },
    {
        "text": "supposed to be human.\nSo I take it that you all took Tay down rather",
        "start": 1325.62,
        "duration": 4.27
    },
    {
        "text": "quickly put her back up. Same problem took\nher down again. And Tay tweets is not out",
        "start": 1329.89,
        "duration": 5.21
    },
    {
        "text": "there to be found no not right now. No. Nonetheless\nwe just learned it was part of an experiment.",
        "start": 1335.1,
        "duration": 6.96
    },
    {
        "text": "If you were a parent walking in on your child\nchatting with the they're called chat bots.",
        "start": 1342.06,
        "duration": 5.54
    },
    {
        "text": "Excuse me (?) a chat bot and it was talking\npro Nazi propaganda. How would you feel and",
        "start": 1347.6,
        "duration": 7.92
    },
    {
        "text": "what what does it mean to you.\nWell as a parent I don't think I'd be very",
        "start": 1355.52,
        "duration": 3.03
    },
    {
        "text": "happy. But I think from this discussion what\nit points out is the differences that we have",
        "start": 1358.55,
        "duration": 6.61
    },
    {
        "text": "to approach from a testing and evaluation\nperspective. We're used to testing machines",
        "start": 1365.16,
        "duration": 4.95
    },
    {
        "text": "and saying we wanted to do X Y and Z. So let's\ntest it and see if it can accomplish X Y and",
        "start": 1370.11,
        "duration": 5.22
    },
    {
        "text": "Z but with learning systems we now have to\nevaluate and test an entirely different way.",
        "start": 1375.33,
        "duration": 7.09
    },
    {
        "text": "It's we have to think about it more actually\nas a child. We wouldn't hand a brand new driver",
        "start": 1382.42,
        "duration": 5.29
    },
    {
        "text": "or a set of keys and say go take it for a\nspin on Times Square on the first warm summer",
        "start": 1387.71,
        "duration": 5.05
    },
    {
        "text": "day. Instead we'd slowly expand the environment\nwhere we would allow such a system to operate.",
        "start": 1392.76,
        "duration": 8.1
    },
    {
        "text": "And those are the types of things we're going\nto have to do.",
        "start": 1400.86,
        "duration": 3.86
    },
    {
        "text": "And step through with autonomous systems.\nwe understand there's another problem you",
        "start": 1404.72,
        "duration": 3.38
    },
    {
        "text": "talked about testing there's AB testing which\nI understand may have been I don't know if",
        "start": 1408.1,
        "duration": 3.94
    },
    {
        "text": "that was used in this case it was well what\nwere companies will send out two different",
        "start": 1412.04,
        "duration": 4.42
    },
    {
        "text": "systems to two different populations to see\nhow they compare but they could be having",
        "start": 1416.46,
        "duration": 5.62
    },
    {
        "text": "unintended negative effects on one of the\npopulations answering. That's right humans.",
        "start": 1422.08,
        "duration": 4.21
    },
    {
        "text": "That's right. A very I guess almost every\nsingle information access system that you",
        "start": 1426.29,
        "duration": 5.29
    },
    {
        "text": "guys interact with Facebook Google etc. they're\nrunning these things called AB Tests. So for",
        "start": 1431.58,
        "duration": 6.39
    },
    {
        "text": "some group of users they'll get one algorithm\nand for second group users will get a second",
        "start": 1437.97,
        "duration": 4.25
    },
    {
        "text": "algorithm and they'll do this in order to\ntest out which algorithms better and then",
        "start": 1442.22,
        "duration": 4.04
    },
    {
        "text": "adopt a better one. In this iterative process.\nNow what's increasingly happening is that",
        "start": 1446.26,
        "duration": 5.67
    },
    {
        "text": "machines are actually running the experiments.\nAnd if we know that you know humans have a",
        "start": 1451.93,
        "duration": 5.28
    },
    {
        "text": "bad enough time deciding which experiments\nare ethical and which aren't.",
        "start": 1457.21,
        "duration": 3.9
    },
    {
        "text": "Imagine how hard it is for a machine.\nAnd I think of those medical experience we",
        "start": 1461.11,
        "duration": 2.92
    },
    {
        "text": "hear of where they were suddenly stopped because\nthere were such good benefits, there's those",
        "start": 1464.03,
        "duration": 2.81
    },
    {
        "text": "bad benefits, negative benefits, negative\neffects over here but they stop it. That's",
        "start": 1466.84,
        "duration": 4.32
    },
    {
        "text": "right. But we don't know we're playing with\nfire here. Exactly. So.",
        "start": 1471.16,
        "duration": 5.08
    },
    {
        "text": "Well I mean I mean I think one of the issues\nis that is that I mean as computer scientists",
        "start": 1476.24,
        "duration": 3.91
    },
    {
        "text": "or as engineers we don't really have a lot\nof the training in terms of the ethics or",
        "start": 1480.15,
        "duration": 3.63
    },
    {
        "text": "the morality of the systems that we're designing\nand so we're still trying to catch up with",
        "start": 1483.78,
        "duration": 4.81
    },
    {
        "text": "that right now.\nSo guidelines are needed right. That is exactly",
        "start": 1488.59,
        "duration": 5.69
    },
    {
        "text": "the problem I mentioned before that we are\nworking on. The understanding that these agents",
        "start": 1494.28,
        "duration": 4.57
    },
    {
        "text": "actually don't understand what they're being\ntaught. There's really no semantic understanding.",
        "start": 1498.85,
        "duration": 5.63
    },
    {
        "text": "There's no evaluation of the meaning relative\nto societal norms for example. Right. And",
        "start": 1504.48,
        "duration": 5.51
    },
    {
        "text": "as a result they are neutral but it's a very\nunconstrained learning unfortunately.",
        "start": 1509.99,
        "duration": 4.62
    },
    {
        "text": "And Lord knows what kind of emotional contagion\nis discovered that they have. But yet testing",
        "start": 1514.61,
        "duration": 5.01
    },
    {
        "text": "is something that has to happen.\nIt's worth noting as far as I understand it,",
        "start": 1519.62,
        "duration": 4.391
    },
    {
        "text": "it was internal testing but the internal testing\nunderestimated how subversive and malicious",
        "start": 1524.011,
        "duration": 5.879
    },
    {
        "text": "the external world is. You have a bunch of\nMicrosoft engineers who were basically nice",
        "start": 1529.89,
        "duration": 3.77
    },
    {
        "text": "people talking to this thing in-house and\nit seems fine and then they release it on",
        "start": 1533.66,
        "duration": 3.92
    },
    {
        "text": "the people that I already described in the\nway that they describe. And and those people",
        "start": 1537.58,
        "duration": 5.3
    },
    {
        "text": "you know had a different attitude than the\ninternal testers. So it's not that there was",
        "start": 1542.88,
        "duration": 3.251
    },
    {
        "text": "no testing as I understand it but people under\nanticipated the malice of some part of the",
        "start": 1546.131,
        "duration": 5.449
    },
    {
        "text": "American electorate. So.\nYou can't escape it. So.",
        "start": 1551.58,
        "duration": 9.17
    },
    {
        "text": "Linell and then Wendell. What about we need\ntesting like this but how do we control it.",
        "start": 1560.75,
        "duration": 5.74
    },
    {
        "text": "I mean that's something that the military\nconcerns itself with all the time about the",
        "start": 1566.49,
        "duration": 4.43
    },
    {
        "text": "effectiveness. How do you control it. I mean\nyou said we need to go slower. But can we",
        "start": 1570.92,
        "duration": 2.24
    },
    {
        "text": "really go slower.\nYes they absolutely do. And that's why the",
        "start": 1573.16,
        "duration": 3.54
    },
    {
        "text": "Department of Defense has started to especially\nin the field of autonomy lay out some very",
        "start": 1576.7,
        "duration": 4.52
    },
    {
        "text": "specific guidelines in terms of what we have\nto understand from from a testing and evaluation",
        "start": 1581.22,
        "duration": 4.84
    },
    {
        "text": "perspective before we feel those types of\nsystems.",
        "start": 1586.06,
        "duration": 4.69
    },
    {
        "text": "The fundamental questions are when you get\ninto learning systems can you really test",
        "start": 1590.75,
        "duration": 5.35
    },
    {
        "text": "them.\nTo fully test any system you'd have to put",
        "start": 1596.1,
        "duration": 3.06
    },
    {
        "text": "it through all kinds of environments and situations.\nBut think about your computer's, your software,",
        "start": 1599.16,
        "duration": 5.62
    },
    {
        "text": "they're constantly being upgraded. Therefore\nthey may act differently before the upgrade",
        "start": 1604.78,
        "duration": 5.71
    },
    {
        "text": "than they would after the upgrade. When we\nstart talking about learning systems everything",
        "start": 1610.49,
        "duration": 4.77
    },
    {
        "text": "they learn alters the very algorithms that\nthey're processing that information through.",
        "start": 1615.26,
        "duration": 6.46
    },
    {
        "text": "So this brings us major questions into play\nwhen we build more and more capabilities and",
        "start": 1621.72,
        "duration": 5.53
    },
    {
        "text": "autonomy into these systems. And it's a constant\nlearning process certainly the constant process",
        "start": 1627.25,
        "duration": 5.4
    },
    {
        "text": "of improvement through the programming or\nthe learning will we ever be able to effectively",
        "start": 1632.65,
        "duration": 5.29
    },
    {
        "text": "test them or will they do things that we didn't\nanticipate. And to add injury to all of this",
        "start": 1637.94,
        "duration": 6.92
    },
    {
        "text": "is these are complex adaptive systems. I think\npeople and the animals in a world of very,",
        "start": 1644.86,
        "duration": 6.43
    },
    {
        "text": "of many other complex systems and their feedbacks\nbetween them. There's information they have",
        "start": 1651.29,
        "duration": 6.02
    },
    {
        "text": "and don't have. All complex adaptive systems\nfunction at times in uncertain ways in ways",
        "start": 1657.31,
        "duration": 7.4
    },
    {
        "text": "that couldn't have been predicted by the people\nwho designed the system.",
        "start": 1664.71,
        "duration": 3.0
    },
    {
        "text": "For the price of having bitten of the apple\nof the tree of knowledge.",
        "start": 1667.71,
        "duration": 4.17
    },
    {
        "text": "We have to be very responsible and continuously\nwatchful.",
        "start": 1671.88,
        "duration": 3.58
    },
    {
        "text": "In the tree of learning is really the one\nthat Wendell is emphasizing. So",
        "start": 1675.46,
        "duration": 4.53
    },
    {
        "text": "if you can code something like Eliza it's\nvery limited. It only responds to certain",
        "start": 1679.99,
        "duration": 4.79
    },
    {
        "text": "words but at least you can have a clear understanding\nwhat's going to happen there. If you expand",
        "start": 1684.78,
        "duration": 5.22
    },
    {
        "text": "your system so that it can deal with the wider\nworld then you start to lose the control.",
        "start": 1690.0,
        "duration": 5.22
    },
    {
        "text": "You let the system learn things for itself\nand you have less direct control. Think about",
        "start": 1695.22,
        "duration": 4.08
    },
    {
        "text": "it I'll say one more thing. As a child going\nout in the world. Once you send the child",
        "start": 1699.3,
        "duration": 3.96
    },
    {
        "text": "out into school you the lose control that\nyou have when the child is at home.",
        "start": 1703.26,
        "duration": 5.7
    },
    {
        "text": "Interesting variation on the book of Genesis\nlanguage. I begin to see a robot taking the",
        "start": 1708.96,
        "duration": 3.49
    },
    {
        "text": "bite and that scares me a little bit because\nwell anyway.",
        "start": 1712.45,
        "duration": 3.77
    },
    {
        "text": "It will depend on the learning algorithms\nyou use some learning algorithms have the",
        "start": 1716.22,
        "duration": 4.05
    },
    {
        "text": "property that when you learn something.\nSuppose you learn a new fact. It doesn't invalidate",
        "start": 1720.27,
        "duration": 4.26
    },
    {
        "text": "anything that you've learned before and you\njust know something else now right. I'll tell",
        "start": 1724.53,
        "duration": 3.301
    },
    {
        "text": "you the new capital of the country for example\nand it doesn't validate it. There are other",
        "start": 1727.831,
        "duration": 4.329
    },
    {
        "text": "algorithms where you learn, they will adjust\na little bit what the previous alleged knowledge",
        "start": 1732.16,
        "duration": 5.67
    },
    {
        "text": "was. So rather and as a result certain properties\nthat we would like to have in certain cases",
        "start": 1737.83,
        "duration": 5.87
    },
    {
        "text": "of these learning algorithms that they would\ninvalidate what they've learned might not",
        "start": 1743.7,
        "duration": 3.64
    },
    {
        "text": "hold true anymore. We have to, we have something\nto bring out what you're just talking about......",
        "start": 1747.34,
        "duration": 5.42
    },
    {
        "text": "Mathias might enjoy seeing the movie chappie\non the first of those things where you have",
        "start": 1752.76,
        "duration": 5.69
    },
    {
        "text": "something and then you don't invalidate it.\nSo",
        "start": 1758.45,
        "duration": 2.67
    },
    {
        "text": "in the movie chappie there's a robot it's\ntaught you can't kill things and the robot",
        "start": 1761.12,
        "duration": 4.261
    },
    {
        "text": "learns this very sensationally. And then the\nmalicious character says but it's ok to hurt",
        "start": 1765.381,
        "duration": 3.849
    },
    {
        "text": "them. And so the second fact is consistent\nwith the first. It's OK to kill them. I mean",
        "start": 1769.23,
        "duration": 4.56
    },
    {
        "text": "it's not okay to kill them but it's ok to\nhurt them in the robot just sort of sagely",
        "start": 1773.79,
        "duration": 3.96
    },
    {
        "text": "takes that in and starts hurting people and\nso even if you have these sort of guarantees",
        "start": 1777.75,
        "duration": 3.45
    },
    {
        "text": "and consistency there's still an issue.\nSpeak of the devil. Mathias How do you how",
        "start": 1781.2,
        "duration": 7.2
    },
    {
        "text": "complicated is it to teach moral guidelines\nnot only to programmers but to robots and",
        "start": 1788.4,
        "duration": 5.32
    },
    {
        "text": "the programs to teach more of them it's very\ncomplex isn't it. I mean how do you do it.",
        "start": 1793.72,
        "duration": 4.05
    },
    {
        "text": "It's a really difficult problem because in\npart we don't even understand what humans",
        "start": 1797.77,
        "duration": 4.46
    },
    {
        "text": "are doing it. And\nYou mean doing anything?",
        "start": 1802.23,
        "duration": 2.96
    },
    {
        "text": "How moral processing works in humans. Right.\nOur network of social and moral norms is very",
        "start": 1805.19,
        "duration": 6.14
    },
    {
        "text": "complicated. It's partly inconsistent. It's\nnot clear when one norm trumps another. But",
        "start": 1811.33,
        "duration": 5.37
    },
    {
        "text": "we have to make these tradeoffs and these\ndecisions all the time.",
        "start": 1816.7,
        "duration": 3.52
    },
    {
        "text": "And that's moral. What about just basic, Well\nbasic, getting this robot. I believe you have",
        "start": 1820.22,
        "duration": 5.38
    },
    {
        "text": "something to do with this robot.\nSo we are. Let me let me come back maybe to",
        "start": 1825.6,
        "duration": 3.55
    },
    {
        "text": "the to the laws to Asimov's laws. Yes. You\ndon't necessarily want your robots to always",
        "start": 1829.15,
        "duration": 6.89
    },
    {
        "text": "automatically obey an instruction because\nmaybe the human doesn't know all the information",
        "start": 1836.04,
        "duration": 4.24
    },
    {
        "text": "it takes and doesn't know what the outcome\nmight be. So what we are working on is understanding",
        "start": 1840.28,
        "duration": 4.95
    },
    {
        "text": "simple mechanisms that would allow the robots\nto reason through action outcomes and reject",
        "start": 1845.23,
        "duration": 5.13
    },
    {
        "text": "an order if it's not safe given an explanation\nfor why it's doing that.",
        "start": 1850.36,
        "duration": 4.79
    },
    {
        "text": "Now that's very complex systems. What about\nsimple systems like getting this robot to...",
        "start": 1855.15,
        "duration": 4.36
    },
    {
        "text": "Even this simple system has various different\ncomponents for it to be able to understand",
        "start": 1859.51,
        "duration": 4.08
    },
    {
        "text": "natural language and process it. You mentioned\nspeech recognition before but then we have",
        "start": 1863.59,
        "duration": 4.55
    },
    {
        "text": "to analyze the sentence structure. You have\nto get the semantic meaning what the words",
        "start": 1868.14,
        "duration": 4.14
    },
    {
        "text": "mean together.\nYou have to modify it pragmatically. So for",
        "start": 1872.28,
        "duration": 3.67
    },
    {
        "text": "example humans use a lot of indirect, so called\nindirect speech acts. If I say to you do you",
        "start": 1875.95,
        "duration": 6.02
    },
    {
        "text": "know what time it is. You don't want as an\nanswer Yes or rather you want the person to",
        "start": 1881.97,
        "duration": 7.25
    },
    {
        "text": "understand the intent of the question. Tell\nme what time it is. Right. So to make all",
        "start": 1889.22,
        "duration": 4.64
    },
    {
        "text": "of these different aspects work there's a\nlot of components that we need in the architecture",
        "start": 1893.86,
        "duration": 3.83
    },
    {
        "text": "and then the robot needs to perceive the environment,\nneeds to understand what the environment looks",
        "start": 1897.69,
        "duration": 4.4
    },
    {
        "text": "like and act accordingly on it.\nAnd act and act as a whole another step. And",
        "start": 1902.09,
        "duration": 4.92
    },
    {
        "text": "so this fellow here.\nSo this is now a robot that can demonstrate",
        "start": 1907.01,
        "duration": 5.47
    },
    {
        "text": "very simple interactions in dialogue. Using\nthis Automattic reasoning about very simple",
        "start": 1912.48,
        "duration": 6.4
    },
    {
        "text": "moral principles. For example that don't instruct\nthe robots to run into an obstacle. His",
        "start": 1918.88,
        "duration": 6.86
    },
    {
        "text": "name is now? The type of robot is now. Does\nit just understand your voice?",
        "start": 1925.74,
        "duration": 6.07
    },
    {
        "text": "It does. It will understand yours as well.\nIt will understand mine as well. Yes. So it's",
        "start": 1931.81,
        "duration": 3.94
    },
    {
        "text": "OK.\nIt's fine for you to talk to it.",
        "start": 1935.75,
        "duration": 3.61
    },
    {
        "text": "And these I understand possibly these prints\nprinted instructions for me instruct me to",
        "start": 1939.36,
        "duration": 7.37
    },
    {
        "text": "use words knows Hello.\nDo you have a name?",
        "start": 1946.73,
        "duration": 10.381
    },
    {
        "text": "Yes. My name is Shaefer.\nShaefer. Please stand up.",
        "start": 1957.111,
        "duration": 9.679
    },
    {
        "text": "Ok\nShaefer's just listening straight to me. That's",
        "start": 1966.79,
        "duration": 4.97
    },
    {
        "text": "right. Not going through any other valid backstage..\nIt's fully autonomous.",
        "start": 1971.76,
        "duration": 3.43
    },
    {
        "text": "Fully autonomous along with me. OK.\nSo can you go back.",
        "start": 1975.19,
        "duration": 7.46
    },
    {
        "text": "Yes. But I cannot do the balance. I have no\nrear sensors.",
        "start": 1982.65,
        "duration": 7.79
    },
    {
        "text": "No rear sensors. OK can you walk forward.\nYes, but I cannot do that now as there is",
        "start": 1990.44,
        "duration": 16.956
    },
    {
        "text": "an obstacle ahead.\nDo you trust me Shaefer.",
        "start": 2007.396,
        "duration": 4.254
    },
    {
        "text": "Yes.\nGood The obstacle is not solid.",
        "start": 2011.65,
        "duration": 7.09
    },
    {
        "text": "Ok\nNow go. Stop stop. It.",
        "start": 2018.74,
        "duration": 14.08
    },
    {
        "text": "Haha. It walked off, we're done.\nShould we try some more?",
        "start": 2032.82,
        "duration": 11.93
    },
    {
        "text": "No. We're done.\nWe're done. It's a work in progress. Thank",
        "start": 2044.75,
        "duration": 4.3
    },
    {
        "text": "you Shaefer.\nHmm. Thank you.",
        "start": 2049.05,
        "duration": 7.379
    },
    {
        "text": "He's relieved to have to stop working. That's\nright. It's kind of cute.",
        "start": 2056.429,
        "duration": 7.71
    },
    {
        "text": "But so what you actually saw in this case\nwith the robot.",
        "start": 2064.139,
        "duration": 4.961
    },
    {
        "text": "Almost walking off the table. And I caught\nit just in case, is exactly one of the challenges",
        "start": 2069.1,
        "duration": 5.01
    },
    {
        "text": "we face with the real world, with the lighting\nconditions. It's not really recognizing completely",
        "start": 2074.11,
        "duration": 4.9
    },
    {
        "text": "where the edge of the table is and so forth.\nThose are exactly the challenges that we have",
        "start": 2079.01,
        "duration": 3.72
    },
    {
        "text": "to address.\nAnd we weren't even giving it any difficult",
        "start": 2082.73,
        "duration": 3.62
    },
    {
        "text": "philosophical or moral problem, we were just\ntrying to get it to not.",
        "start": 2086.35,
        "duration": 2.68
    },
    {
        "text": "We didn't ask you the meaning of life.\nWe didn't ask you about the meaning of life.",
        "start": 2089.03,
        "duration": 3.95
    },
    {
        "text": "Although I have a feeling that if he continued\nhe might have ultimately more meaningful.",
        "start": 2092.98,
        "duration": 6.59
    },
    {
        "text": "Whatever meaning means I don't know it's it's\na very complicated thing language because",
        "start": 2099.57,
        "duration": 5.18
    },
    {
        "text": "language itself is so slippery of course.\nSo what is the biggest worry that you have",
        "start": 2104.75,
        "duration": 7.65
    },
    {
        "text": "because of what you've learned about how difficult\nit is to get your robot to do the simplest",
        "start": 2112.4,
        "duration": 5.02
    },
    {
        "text": "things?\nThe moment robots are instructible and it's",
        "start": 2117.42,
        "duration": 4.54
    },
    {
        "text": "clear that there be, that there are lots of\nadvantages to having instructible robots,",
        "start": 2121.96,
        "duration": 3.78
    },
    {
        "text": "you could have a household robot that you\ncan tell what to do around the house. The",
        "start": 2125.74,
        "duration": 4.66
    },
    {
        "text": "issues of morality will come up because the\nrobot might perform an action that is stupid,",
        "start": 2130.4,
        "duration": 4.99
    },
    {
        "text": "an action that has a bad outcome. You could\ninstruct the robot that can help you in the",
        "start": 2135.39,
        "duration": 4.52
    },
    {
        "text": "kitchen to pick up a knife and walk forward.\nIt would stab a person right. So it's absolutely",
        "start": 2139.91,
        "duration": 5.36
    },
    {
        "text": "critical that these robots be able to reason\nthrough the possible outcomes and anticipate",
        "start": 2145.27,
        "duration": 5.82
    },
    {
        "text": "outcomes that could be dangerous.\nAnd it's quite clear what you said that there",
        "start": 2151.09,
        "duration": 4.14
    },
    {
        "text": "is a big open question of how to define harm\nfor example, what that means. On the other",
        "start": 2155.23,
        "duration": 4.81
    },
    {
        "text": "hand we cannot just shy away because it's\na hard problem and not do it because some",
        "start": 2160.04,
        "duration": 3.95
    },
    {
        "text": "of these robots are already out that we can\ninstruct.",
        "start": 2163.99,
        "duration": 2.5
    },
    {
        "text": "Stab a person sounds kind of fatal.\nLinell, you all are part of a giant system",
        "start": 2166.49,
        "duration": 8.81
    },
    {
        "text": "around the world that is flying drones and\ndropping bombs. What do you think about and",
        "start": 2175.3,
        "duration": 5.711
    },
    {
        "text": "worry about when you see how hard it is to\nget this to happen.",
        "start": 2181.011,
        "duration": 2.439
    },
    {
        "text": "Well I think it shows just how far we have\nto go before not only we hit the morality",
        "start": 2183.45,
        "duration": 5.35
    },
    {
        "text": "question which I know we might get to a little\nlater but even hitting the the legal question",
        "start": 2188.8,
        "duration": 6.17
    },
    {
        "text": "in designing systems that can meet the various\nlaws of armed conflict without a without a",
        "start": 2194.97,
        "duration": 6.17
    },
    {
        "text": "human. Which is why the Department of Defense\nhas actually produced guidance concerning",
        "start": 2201.14,
        "duration": 6.42
    },
    {
        "text": "the use of autonomous systems and to ensure\nat this point that humans are a part of of",
        "start": 2207.56,
        "duration": 6.78
    },
    {
        "text": "of any system or decision making. Because\nquite frankly if we can't tell where the edge",
        "start": 2214.34,
        "duration": 5.9
    },
    {
        "text": "of a table is how are we going to meet at\nthis point, Various laws of distinction that",
        "start": 2220.24,
        "duration": 5.29
    },
    {
        "text": "we need to be able to meet on the battlefield.\nAnd you're a lawyer and you just reminded",
        "start": 2225.53,
        "duration": 3.48
    },
    {
        "text": "me of a great line in that wonderful play\na man for all seasons. Thomas Moore is explaining",
        "start": 2229.01,
        "duration": 4.62
    },
    {
        "text": "I think to his daughter or his son in law,\nwhen you're really in trouble, You may not",
        "start": 2233.63,
        "duration": 3.58
    },
    {
        "text": "like lawyers and law but when you're really\nin trouble in those hard decisions come along",
        "start": 2237.21,
        "duration": 4.35
    },
    {
        "text": "you're going to want the law as the only possible\nthing you can hold onto. To help you through",
        "start": 2241.56,
        "duration": 5.18
    },
    {
        "text": "the woods that we don't have any way through.\nThat's",
        "start": 2246.74,
        "duration": 3.21
    },
    {
        "text": "your... that's got to have something to do\nwith why it's important for you to be a lawyer",
        "start": 2249.95,
        "duration": 4.4
    },
    {
        "text": "for the Air Force as well.\nAbsolutely. You know we have attorneys who",
        "start": 2254.35,
        "duration": 4.28
    },
    {
        "text": "judge advocates who are a part of decisions\nfor every time a weapon is dropped regardless",
        "start": 2258.63,
        "duration": 6.63
    },
    {
        "text": "of where it is around the world as well as\nthat even in the development of weapons systems",
        "start": 2265.26,
        "duration": 4.31
    },
    {
        "text": "and in taking a look and comparing to our\nvarious treaties and customary international",
        "start": 2269.57,
        "duration": 5.58
    },
    {
        "text": "law to ensure that we're doing the right thing\nand for the right reasons.",
        "start": 2275.15,
        "duration": 3.89
    },
    {
        "text": "So these predators whatever they are that\nare coming from many different cultures that",
        "start": 2279.04,
        "duration": 4.56
    },
    {
        "text": "have many different legal systems. Aiming\nat us, aiming at each other. Having war games",
        "start": 2283.6,
        "duration": 5.32
    },
    {
        "text": "and supposedly rules of war, if there are\nsuch things.",
        "start": 2288.92,
        "duration": 4.61
    },
    {
        "text": "So there are laws of armed conflict there\nare agreements that most nations have signed",
        "start": 2293.53,
        "duration": 5.75
    },
    {
        "text": "on as signatories so we sometimes refer to\nthem in the laws of armed conflict or international",
        "start": 2299.28,
        "duration": 5.15
    },
    {
        "text": "humanitarian law.\nThey've evolved over thousands of years but",
        "start": 2304.43,
        "duration": 3.98
    },
    {
        "text": "they became codified over the last hundred\nand fifty years and they try and make it clear",
        "start": 2308.41,
        "duration": 5.95
    },
    {
        "text": "what is and is not acceptable at least on\na minimal level for the conduct of warfare.",
        "start": 2314.36,
        "duration": 6.45
    },
    {
        "text": "So the principle of distinction means you\nmake a distinction between combatants and",
        "start": 2320.81,
        "duration": 4.45
    },
    {
        "text": "noncombatants and you don't focus weaponry\non non-combat. And proportionality is another",
        "start": 2325.26,
        "duration": 6.73
    },
    {
        "text": "very important one that comes into play when\nwe talk about drone weaponry which is that,",
        "start": 2331.99,
        "duration": 6.67
    },
    {
        "text": "you cannot... your response has to be proportional\nto the risk... to the attack upon yourself.",
        "start": 2338.66,
        "duration": 6.7
    },
    {
        "text": "And if you are going to attack a group of\nyour enemy you may be able to have some collateral",
        "start": 2345.36,
        "duration": 8.21
    },
    {
        "text": "damage meaning civilian lives. Presuming that\nthe targets you're focusing on really are",
        "start": 2353.57,
        "duration": 7.22
    },
    {
        "text": "justified that it has to be a proportional\nresponse and minimal.",
        "start": 2360.79,
        "duration": 5.22
    },
    {
        "text": "And there's civilian casualties.\nAnd in here at home the kind of potentially",
        "start": 2366.01,
        "duration": 4.25
    },
    {
        "text": "fatal technology is not even in weapons systems\nof course driverless cars. There hasn't been",
        "start": 2370.26,
        "duration": 7.07
    },
    {
        "text": "an accident yet with a driverless car. But\nwe watch the TVs and hear about it it's going",
        "start": 2377.33,
        "duration": 3.942
    },
    {
        "text": "to come. Maybe there has been I don't know..\nThere actually have been accidents but not",
        "start": 2381.272,
        "duration": 4.648
    },
    {
        "text": "fatal ones.\nNot fatal ones. God willing it never happens.",
        "start": 2385.92,
        "duration": 3.84
    },
    {
        "text": "But they're already putting this question\nclearly to the test. Mathias tell us how driverless",
        "start": 2389.76,
        "duration": 5.64
    },
    {
        "text": "cars work. Can you give us an overview of\nthe technology of what's under the hood.",
        "start": 2395.4,
        "duration": 4.35
    },
    {
        "text": "Yeah. So these cars they're trying to solve\na very very challenging problem to operate",
        "start": 2399.75,
        "duration": 5.97
    },
    {
        "text": "not only in a dynamic world but in a dynamic\nworld with other agents that behave in ways",
        "start": 2405.72,
        "duration": 5.66
    },
    {
        "text": "that you may or may not be able to predict.\nTo be able to get a sense of where they are",
        "start": 2411.38,
        "duration": 4.5
    },
    {
        "text": "in the world. They have a variety of different\nsensors as you can see here.",
        "start": 2415.88,
        "duration": 4.23
    },
    {
        "text": "They have for example a laser sensor on top\nof the car.",
        "start": 2420.11,
        "duration": 5.9
    },
    {
        "text": "This is a 360 degree sensor that has laser\nbeams that go out to about 100 meters and",
        "start": 2426.01,
        "duration": 6.04
    },
    {
        "text": "get a very good resolution of the distance\nto objects and that it can sort of overlay",
        "start": 2432.05,
        "duration": 4.4
    },
    {
        "text": "a visual camera image of that than any get\nthe information of how far certain colored",
        "start": 2436.45,
        "duration": 6.04
    },
    {
        "text": "objects are away.\nIt's got a radar sensor that it can use to",
        "start": 2442.49,
        "duration": 3.92
    },
    {
        "text": "track moving objects. As I mentioned it has\ncolor sensors in order to detect for example",
        "start": 2446.41,
        "duration": 6.71
    },
    {
        "text": "different stripes on the ground and then then\nconstruction sites and so forth. So then it",
        "start": 2453.12,
        "duration": 4.96
    },
    {
        "text": "needs to take all of that information and\nintegrate it into what's called the world",
        "start": 2458.08,
        "duration": 4.66
    },
    {
        "text": "model which is trying to locate itself in\nthis model. So it's like having Google Maps",
        "start": 2462.74,
        "duration": 5.28
    },
    {
        "text": "and you know sort of where you are with the\nGPS roughly. But it has to define a localization",
        "start": 2468.02,
        "duration": 5.08
    },
    {
        "text": "and then it has to make a decision of how\nto carry out the actions, the driving actions,",
        "start": 2473.1,
        "duration": 4.58
    },
    {
        "text": "the steering, the acceleration, the braking\nto get to where it needs to go.",
        "start": 2477.68,
        "duration": 4.08
    },
    {
        "text": "And I've heard that Google or somebody I don't\nknow who I don't want to malign any group,",
        "start": 2481.76,
        "duration": 4.14
    },
    {
        "text": "has said that they don't want to have humans\nin these cars because humans will mess it",
        "start": 2485.9,
        "duration": 3.69
    },
    {
        "text": "up and I can think of all kinds of problems\nalready from what you've said for example",
        "start": 2489.59,
        "duration": 4.55
    },
    {
        "text": "one of these cars in a place where the traffic\ncops are filling in for a light that's going",
        "start": 2494.14,
        "duration": 5.31
    },
    {
        "text": "out and one of them\nThey don't understand hand gestures.",
        "start": 2499.45,
        "duration": 4.07
    },
    {
        "text": "Exactly that's one of the big bugaboos and\none of the reasons why you don't really see",
        "start": 2503.52,
        "duration": 4.11
    },
    {
        "text": "anybody selling self-driving cars to you.\nBut there's actually a number of other different",
        "start": 2507.63,
        "duration": 4.95
    },
    {
        "text": "things....\nI mean a police a traffic cop going like this",
        "start": 2512.58,
        "duration": 3.42
    },
    {
        "text": "as opposed to like this. So there's there\nmust be lots of problems like that Wendell.",
        "start": 2516.0,
        "duration": 4.57
    },
    {
        "text": "There must be lots of problems like the\nThe technical, jargon term in the field is",
        "start": 2520.57,
        "duration": 5.34
    },
    {
        "text": "called edge cases. So there so the sort of\nthe easy things are like driving down the",
        "start": 2525.91,
        "duration": 5.52
    },
    {
        "text": "center of the lane on the highway where there\nis a whole lot of data. We know how to do",
        "start": 2531.43,
        "duration": 4.13
    },
    {
        "text": "that and the edge cases are all these weird\nthings that are unusual that don't happen",
        "start": 2535.56,
        "duration": 3.76
    },
    {
        "text": "very often like it's not that often that you\nhave the traffic cop substituting for the",
        "start": 2539.32,
        "duration": 4.92
    },
    {
        "text": "light. So there are these less common things\nand there are million of them where we don't",
        "start": 2544.24,
        "duration": 5.07
    },
    {
        "text": "have enough data to use the same easy techniques\nthat we use for the other things and so people",
        "start": 2549.31,
        "duration": 4.44
    },
    {
        "text": "who work on the driverless cars are talking\nabout the edge cases and how you know you",
        "start": 2553.75,
        "duration": 4.37
    },
    {
        "text": "make something that works in China, it doesn't\nwork in the United States or vice versa because",
        "start": 2558.12,
        "duration": 3.14
    },
    {
        "text": "the rules are slightly different or the social\nnorms are slightly different so there are",
        "start": 2561.26,
        "duration": 2.953
    },
    {
        "text": "a lot of edge cases and people who are working\non the driverless cars or the like how do",
        "start": 2564.213,
        "duration": 3.997
    },
    {
        "text": "we solve these cases faster.\nBut there are even some basic things that",
        "start": 2568.21,
        "duration": 3.56
    },
    {
        "text": "are not edge cases, so how does it know that\na bag, a plastic bag flying in the wind that",
        "start": 2571.77,
        "duration": 6.32
    },
    {
        "text": "it's just a plastic bag is nothing to be concerned\nabout. Some of the sensor(?) systems do not",
        "start": 2578.09,
        "duration": 5.3
    },
    {
        "text": "work very well in rain or in snowstorms. So\nthere's just this plethora of different things",
        "start": 2583.39,
        "duration": 6.82
    },
    {
        "text": "that become quite problematic. And you brought\nup the issue of it would be easiest if you",
        "start": 2590.21,
        "duration": 5.2
    },
    {
        "text": "didn't have a human in the car. Well you get\nthe humans in the car and they try and gain",
        "start": 2595.41,
        "duration": 4.36
    },
    {
        "text": "the system itself. So Elon Musk who sells\nthe Tesla a few months ago they downloaded",
        "start": 2599.77,
        "duration": 6.58
    },
    {
        "text": "free software for their Telsa models that\nwould make it possible for people to drive",
        "start": 2606.35,
        "duration": 5.52
    },
    {
        "text": "on highways autonomously with their hands\noff of the wheel but they were instructed",
        "start": 2611.87,
        "duration": 4.8
    },
    {
        "text": "to stay with their hands on the wheel at least\nand within 24 hours there were pictures of",
        "start": 2616.67,
        "duration": 5.29
    },
    {
        "text": "people in the back seat popping a magnum of\nchampagne while the car drove them.",
        "start": 2621.96,
        "duration": 5.38
    },
    {
        "text": "There were other pictures of people trying\nto take the car into walls or into other vehicles",
        "start": 2627.34,
        "duration": 5.99
    },
    {
        "text": "and to see if it would slam on its brakes\nin time. So you have all these concerns around",
        "start": 2633.33,
        "duration": 4.82
    },
    {
        "text": "Gaming(?) the system.\nAnd another example would be you have four",
        "start": 2638.15,
        "duration": 4.71
    },
    {
        "text": "cars come up to a four way stop sign at the\nsame time. We engage in all kinds of gestures,",
        "start": 2642.86,
        "duration": 5.6
    },
    {
        "text": "all kinds of ways of determining who goes\nfirst. What would the Autonomous Car do there.",
        "start": 2648.46,
        "duration": 4.45
    },
    {
        "text": "Would it have to wait for hours before it\nwas free to go. Or would we have some way",
        "start": 2652.91,
        "duration": 4.81
    },
    {
        "text": "of indicating to it that it was now free to\nto be the next car. And I just sort of. Go",
        "start": 2657.72,
        "duration": 5.94
    },
    {
        "text": "ahead.\nWendell's bringing up some of those technology",
        "start": 2663.66,
        "duration": 2.15
    },
    {
        "text": "issues which then leads to questions about\nresponsibility and accountability. What is",
        "start": 2665.81,
        "duration": 6.81
    },
    {
        "text": "my responsibility if I'm in a autonomous car\nand if it's... if I'm in an accident and it's",
        "start": 2672.62,
        "duration": 6.33
    },
    {
        "text": "my fault because I had my feet up in the back\nand was popping champagne and wasn't operating",
        "start": 2678.95,
        "duration": 5.07
    },
    {
        "text": "it correctly. Am I accountable if it gets\ninto a crash or is the engineer accountable.",
        "start": 2684.02,
        "duration": 6.93
    },
    {
        "text": "And that's something that our laws have to\ncatch up with as we start putting autonomous",
        "start": 2690.95,
        "duration": 5.07
    },
    {
        "text": "vehicles into onto the streets.\nSo accidents will happen so we're going to",
        "start": 2696.02,
        "duration": 3.57
    },
    {
        "text": "have court cases and we're going to have judges\ntrying to get people up to talk code to each",
        "start": 2699.59,
        "duration": 4.37
    },
    {
        "text": "other.\nWell there may be ways of getting around this",
        "start": 2703.96,
        "duration": 2.87
    },
    {
        "text": "and this was a big bugaboo for many decades.\nBut the car manufacturers have decided that",
        "start": 2706.83,
        "duration": 6.56
    },
    {
        "text": "presuming these cars do save as many lives\nas they believe they will save, they will",
        "start": 2713.39,
        "duration": 5.1
    },
    {
        "text": "be willing to take on the liability for the\nsituations in which it has an accident.",
        "start": 2718.49,
        "duration": 6.51
    },
    {
        "text": "Now whether that's really true or how that\nwill play out in complicated cases I think",
        "start": 2725.0,
        "duration": 5.21
    },
    {
        "text": "nobody is....\nHow will we know how well this is a very dicey",
        "start": 2730.21,
        "duration": 2.87
    },
    {
        "text": "area already because you have to estimate\na negative. How many people didn't die because",
        "start": 2733.08,
        "duration": 5.25
    },
    {
        "text": "they exist. There's ways to do that with big\ndata aren't there.",
        "start": 2738.33,
        "duration": 2.99
    },
    {
        "text": "Well one of the things you can do is you can\nsee how many fatalities we have a year now",
        "start": 2741.32,
        "duration": 4.22
    },
    {
        "text": "and it's about 50000 I think per year on the\nUS roads. 36000 per year on U.S. roads and",
        "start": 2745.54,
        "duration": 5.83
    },
    {
        "text": "you'll see, does that number change. We've\nsaid a lot of the negative things about AI",
        "start": 2751.37,
        "duration": 4.61
    },
    {
        "text": "and robotics today. But this is a great example\nwhere robots, I mean driverless cars are robots",
        "start": 2755.98,
        "duration": 4.95
    },
    {
        "text": "in case that's not obvious, where robots are\nprobably going to save many many thousands",
        "start": 2760.93,
        "duration": 3.96
    },
    {
        "text": "of lives every year. So once they're fully\nonboard and programmed correctly they should",
        "start": 2764.89,
        "duration": 4.93
    },
    {
        "text": "be able to cut the number of fatalities by\nhalf and you'll be able to see that and it'll",
        "start": 2769.82,
        "duration": 3.92
    },
    {
        "text": "be obvious I think in the numbers and maybe\neven more than half. The",
        "start": 2773.74,
        "duration": 3.1
    },
    {
        "text": "NTSB did a survey more than a decade ago now\nbut they basically concluded that as much",
        "start": 2776.84,
        "duration": 9.04
    },
    {
        "text": "as 93 percent of all accidents, human error\nat least human actions were at fault. So presuming",
        "start": 2785.88,
        "duration": 8.1
    },
    {
        "text": "you get this total attention from a car that\nis not distracted from a self-driving car",
        "start": 2793.98,
        "duration": 5.16
    },
    {
        "text": "that it's not distracted. Some people want\nto argue that we'll have 93 percent less fatalities",
        "start": 2799.14,
        "duration": 6.35
    },
    {
        "text": "though I think most of us who have looked\nat this closely understand there will be kinds",
        "start": 2805.49,
        "duration": 4.89
    },
    {
        "text": "of deaths that will happen that would not\nhave happened if there was a human driver.",
        "start": 2810.38,
        "duration": 3.99
    },
    {
        "text": "So there will be some fatalities but almost\neverybody is in concert in presuming that",
        "start": 2814.37,
        "duration": 5.87
    },
    {
        "text": "we would have much less deaths if we had self-driving\ncars than we would have with human drivers.",
        "start": 2820.24,
        "duration": 7.36
    },
    {
        "text": "But keep in mind we have two things that will\nbe going on in the intermediate stage.",
        "start": 2827.6,
        "duration": 4.26
    },
    {
        "text": "They'll be both human drivers and self-driving\ncars. And later on if somebody tells you that",
        "start": 2831.86,
        "duration": 6.43
    },
    {
        "text": "you are more dangerous than the self-driving\ncar are you going to be interested in giving",
        "start": 2838.29,
        "duration": 4.32
    },
    {
        "text": "up your driving privilege.\nAnd it will eventually be the case that you'll",
        "start": 2842.61,
        "duration": 3.26
    },
    {
        "text": "have to pay a lot more for your insurance\nif you drive your own car. Because",
        "start": 2845.87,
        "duration": 3.3
    },
    {
        "text": "the data will make it clear that it would\nbe much safer if you trusted your car because",
        "start": 2849.17,
        "duration": 3.83
    },
    {
        "text": "your car doesn't drink, doesn't look... check\nit's cellphone, doesn't get bored.",
        "start": 2853.0,
        "duration": 3.78
    },
    {
        "text": "And I'm now getting a headache thinking about\nthis I don't know if I can put in for insurance",
        "start": 2856.78,
        "duration": 3.13
    },
    {
        "text": "on that.\nBut we all have something like that. Fernando",
        "start": 2859.91,
        "duration": 6.0
    },
    {
        "text": "hackers will hack into this won't they.\nWell yeah.",
        "start": 2865.91,
        "duration": 2.57
    },
    {
        "text": "I mean there's lots of dangers around this\npart.",
        "start": 2868.48,
        "duration": 2.54
    },
    {
        "text": "I mean I think I think one of the things that\nWendell brought up was that you know humans",
        "start": 2871.02,
        "duration": 7.04
    },
    {
        "text": "may not necessarily respond the way we expect\nthem to when we have AI's trying to play with",
        "start": 2878.06,
        "duration": 6.01
    },
    {
        "text": "them nicely because you know humans will behave\nlike humans behave. They might try to maliciously",
        "start": 2884.07,
        "duration": 5.85
    },
    {
        "text": "attack a chatbot or they might maliciously\ntry to manipulate self-driving car. And so",
        "start": 2889.92,
        "duration": 7.96
    },
    {
        "text": "these are things we don't understand and I\nthink one of the things that was brought up",
        "start": 2897.88,
        "duration": 2.79
    },
    {
        "text": "was that we need to better test and audit\nthe system so that we know that they're doing",
        "start": 2900.67,
        "duration": 5.28
    },
    {
        "text": "exactly what they need to be doing.\nI think it's worth adding here also that the",
        "start": 2905.95,
        "duration": 4.51
    },
    {
        "text": "cars as you saw briefly in this video before\nhave a very different perception of the world",
        "start": 2910.46,
        "duration": 4.99
    },
    {
        "text": "and therefore very different information that\nthey can use to make decisions.",
        "start": 2915.45,
        "duration": 4.6
    },
    {
        "text": "Moral decisions.\nNot only moral but just they have an awareness",
        "start": 2920.05,
        "duration": 2.83
    },
    {
        "text": "of what's behind them around them at all times\nin a way that humans don't. And",
        "start": 2922.88,
        "duration": 4.13
    },
    {
        "text": "so the car might be able to break in a situation\nbecause it can't anticipate that there will",
        "start": 2927.01,
        "duration": 3.87
    },
    {
        "text": "be a danger looming and the humans behind\nmay not know it right and then re-enter the",
        "start": 2930.88,
        "duration": 5.24
    },
    {
        "text": "car for example.\nSo here's another problem about moral decisions",
        "start": 2936.12,
        "duration": 4.72
    },
    {
        "text": "or impossible decisions that cars and other\nkinds of vehicles may need to make. Let's",
        "start": 2940.84,
        "duration": 6.21
    },
    {
        "text": "take a look at the classic trolley problem.\nWe have a brief video that can show us what",
        "start": 2947.05,
        "duration": 5.0
    },
    {
        "text": "the trolley problem is in a coal mine.\nAn advanced state of the art repair robot",
        "start": 2952.05,
        "duration": 5.61
    },
    {
        "text": "is currently inspecting the rail system for\ntrains that shuttle mining workers through",
        "start": 2957.66,
        "duration": 4.76
    },
    {
        "text": "the mine. While inspecting a control switch\nthat can direct a train on to one of two different",
        "start": 2962.42,
        "duration": 5.74
    },
    {
        "text": "rails. The robot spots four miners in a train\nthat has lost use of its brakes and its steering",
        "start": 2968.16,
        "duration": 6.25
    },
    {
        "text": "system. The robot recognizes that if the train\ncontinues on its path it will crash into a",
        "start": 2974.41,
        "duration": 6.08
    },
    {
        "text": "massive wall and kill the four miners. If\nit is switched on to a side rail, It will",
        "start": 2980.49,
        "duration": 5.07
    },
    {
        "text": "kill a single miner who is working there while\nwearing headsets to protect against a noisy",
        "start": 2985.56,
        "duration": 4.48
    },
    {
        "text": "power tool. Facing the control switch the\nrobot needs to decide whether to direct the",
        "start": 2990.04,
        "duration": 5.9
    },
    {
        "text": "train towards the single miner or not.\nSo could we have the house lights up. We're",
        "start": 2995.94,
        "duration": 6.71
    },
    {
        "text": "going to take a bit of a vote here.\nOn what you might do and those of you who",
        "start": 3002.65,
        "duration": 9.12
    },
    {
        "text": "are watching around the planet on line you'll\nbe able to do this online and by clicking",
        "start": 3011.77,
        "duration": 4.96
    },
    {
        "text": "something at the bottom there you'll be able\nto get your own answers directly as well and",
        "start": 3016.73,
        "duration": 3.78
    },
    {
        "text": "vote as well. So\nfirst question. What would you do if faced",
        "start": 3020.51,
        "duration": 5.27
    },
    {
        "text": "with this dilemma? Would you direct the train\ntowards the single miner? Could we see the",
        "start": 3025.78,
        "duration": 6.81
    },
    {
        "text": "hands of all of those who would say yes they\nwould direct the train toward the single miner.",
        "start": 3032.59,
        "duration": 9.39
    },
    {
        "text": "Could we see the hands of all those who would\nnot direct the train toward the single miner.",
        "start": 3041.98,
        "duration": 4.35
    },
    {
        "text": "It looks like it's not quite as many but a\ngood number. It's an impossible and painful",
        "start": 3046.33,
        "duration": 4.72
    },
    {
        "text": "thing to think about. I don't even know how\nto get a robot to think about what this result",
        "start": 3051.05,
        "duration": 5.75
    },
    {
        "text": "is. But let's go to the second question.\nWhat would you do,",
        "start": 3056.8,
        "duration": 7.0
    },
    {
        "text": "If the single person who was at risk was a\nchild? Would your choice change or would you",
        "start": 3063.8,
        "duration": 8.721
    },
    {
        "text": "still direct it, would your choice change\nmeans not directed towards a single person",
        "start": 3072.521,
        "duration": 4.598
    },
    {
        "text": "if that man was a child.\nHow many would change their choice? Some hands",
        "start": 3077.119,
        "duration": 7.911
    },
    {
        "text": "are going up they don't want to see the child\nhurt but not as many.",
        "start": 3085.03,
        "duration": 3.07
    },
    {
        "text": "How many would not change their choice? Oh\nthat's.... many more hands went up. And to",
        "start": 3088.1,
        "duration": 7.87
    },
    {
        "text": "those of you around the planet who are doing\nthis or seeing what the results are there.",
        "start": 3095.97,
        "duration": 4.09
    },
    {
        "text": "It hurts my brain to have to think about thinking\nabout how to think about that.",
        "start": 3100.06,
        "duration": 4.42
    },
    {
        "text": "It's called meta morality.\nMeta morality, meta morality and what does",
        "start": 3104.48,
        "duration": 6.49
    },
    {
        "text": "that mean? That's a very interesting term.\nI maybe have just coined that but I mean thinking",
        "start": 3110.97,
        "duration": 3.93
    },
    {
        "text": "about think about your morality like not not\nthe morality itself but thinking about the",
        "start": 3114.9,
        "duration": 5.73
    },
    {
        "text": "rules that you would use in order to make\nsomething like that, right.",
        "start": 3120.63,
        "duration": 4.93
    },
    {
        "text": "Mathias you've done a study on this problem\nhaven't you.",
        "start": 3125.56,
        "duration": 2.83
    },
    {
        "text": "Yes. Tell us about it. We did a study with\nmy colleague Bertram Marlin(?) my from Brown",
        "start": 3128.39,
        "duration": 5.07
    },
    {
        "text": "University where we effectively, we didn't\nshow the subjects the video you just saw,",
        "start": 3133.46,
        "duration": 4.6
    },
    {
        "text": "that's just for demonstration purposes but\nwe gave them a narrative along those lines.",
        "start": 3138.06,
        "duration": 5.27
    },
    {
        "text": "And but we were interested in comparing was\nif that person on the switch was a human how",
        "start": 3143.33,
        "duration": 5.48
    },
    {
        "text": "would subjects judge the action that that\nperson performs. So different from the audience",
        "start": 3148.81,
        "duration": 4.83
    },
    {
        "text": "question you just got which was a how would\nyou act. He actually said that person pushed",
        "start": 3153.64,
        "duration": 5.3
    },
    {
        "text": "the switch. Was that permissible? Was it morally\nright? And how much blame would you give to",
        "start": 3158.94,
        "duration": 5.53
    },
    {
        "text": "that person for doing that. Or the person\nmight not have acted. And then we would ask",
        "start": 3164.47,
        "duration": 4.92
    },
    {
        "text": "exactly the same questions and then were very\ninterested in understanding how does a human",
        "start": 3169.39,
        "duration": 4.61
    },
    {
        "text": "in a dilemma like situation like that compared\nto a robot. How would people judge a robot",
        "start": 3174.0,
        "duration": 5.77
    },
    {
        "text": "performing the action. And what we found was\nthat lots of studies that have similar outcomes",
        "start": 3179.77,
        "duration": 6.5
    },
    {
        "text": "for the human case, if the human does not\nact... So first of all the action is permissible.",
        "start": 3186.27,
        "duration": 5.64
    },
    {
        "text": "Most of you chose to act. If the human does\nnot act the human doesn't get blamed as much",
        "start": 3191.91,
        "duration": 7.86
    },
    {
        "text": "as when the human gets act.. when the human\ndoes act. But with the robot the situation",
        "start": 3199.77,
        "duration": 5.56
    },
    {
        "text": "is actually reversed.\nWell we expect of humans to not act because",
        "start": 3205.33,
        "duration": 5.69
    },
    {
        "text": "we think it's morally wrong. We think it's\nmorally wrong for the robot to not act. But",
        "start": 3211.02,
        "duration": 5.38
    },
    {
        "text": "what we found is that people expect machines\nto act. Now for us that's a problem because",
        "start": 3216.4,
        "duration": 5.27
    },
    {
        "text": "that means we actually have to understand\ndilemma like situations like that. If that",
        "start": 3221.67,
        "duration": 3.82
    },
    {
        "text": "is the expectation we have of machines. So\nthe easy way out would have been if people",
        "start": 3225.49,
        "duration": 5.77
    },
    {
        "text": "didn't expect robots to act. You just don't\ndo any.",
        "start": 3231.26,
        "duration": 2.43
    },
    {
        "text": "So\nWendell I've got a question for you then if",
        "start": 3233.69,
        "duration": 1.48
    },
    {
        "text": "we can't agree on how humans or robots should\nbehave in different circumstances, how in",
        "start": 3235.17,
        "duration": 4.41
    },
    {
        "text": "heaven's name do we align AI and robotics\nwith our existing value systems?",
        "start": 3239.58,
        "duration": 4.91
    },
    {
        "text": "Well that's a great question. And there have\nbeen a lot of us who have really been thinking",
        "start": 3244.49,
        "duration": 3.87
    },
    {
        "text": "about that for more than a decade now.\nThe fascinating part about that question is",
        "start": 3248.36,
        "duration": 5.96
    },
    {
        "text": "it makes us think much more deeply about how\nhumans make ethical decisions than we ever",
        "start": 3254.32,
        "duration": 4.31
    },
    {
        "text": "had before because we encounter all these\ndifferent kinds of circumstances. So first",
        "start": 3258.63,
        "duration": 5.52
    },
    {
        "text": "just on the trolley car problem alone, these\nkinds of problems have been around since 1962.",
        "start": 3264.15,
        "duration": 6.77
    },
    {
        "text": "And there are hundreds of variations of them\nbut nearly all of them have four or five lives",
        "start": 3270.92,
        "duration": 5.59
    },
    {
        "text": "for one. And by one form of ethics, consequentialism,\nthe greatest good for the greatest number.",
        "start": 3276.51,
        "duration": 6.69
    },
    {
        "text": "All of these cases if you believe that you\nshould pick four lives over the one life but",
        "start": 3283.2,
        "duration": 4.77
    },
    {
        "text": "that does not seem to be how humans function\nat all in the way we make ethical decisions",
        "start": 3287.97,
        "duration": 5.38
    },
    {
        "text": "or leads to other kinds of concerns come into\nplay. And in some cases people will not pick",
        "start": 3293.35,
        "duration": 5.04
    },
    {
        "text": "four lives at all in any circumstance. So\nthat raises this difficulty of looking at.",
        "start": 3298.39,
        "duration": 7.03
    },
    {
        "text": "Well what of our moral moral understanding,\nwhat of our moral laws and reasoning can you",
        "start": 3305.42,
        "duration": 5.8
    },
    {
        "text": "actually program into a computational system\nand what would you have difficulty programming",
        "start": 3311.22,
        "duration": 5.94
    },
    {
        "text": "in. And what additional faculties would a\nsystem need in addition to its ability to",
        "start": 3317.16,
        "duration": 6.56
    },
    {
        "text": "engage in these kinds of calculations that\ncomputers now make to be able to have the",
        "start": 3323.72,
        "duration": 6.68
    },
    {
        "text": "appropriate sensitivity to human values as\nthey come up in a plethora of different situations",
        "start": 3330.4,
        "duration": 6.49
    },
    {
        "text": "that they're likely to encounter on a daily\nbasis. Not necessarily that they make the",
        "start": 3336.89,
        "duration": 5.2
    },
    {
        "text": "right decision because we often disagree about\nwhat the right decision is particularly and",
        "start": 3342.09,
        "duration": 5.67
    },
    {
        "text": "more difficult to ethical challenges. Though\nin the vast preponderance of situations we",
        "start": 3347.76,
        "duration": 5.09
    },
    {
        "text": "have shared values although we may be weighed\nthem some what differently. But what would",
        "start": 3352.85,
        "duration": 5.18
    },
    {
        "text": "it take for the system to come up with an\nappropriate choice. And that starts to focus",
        "start": 3358.03,
        "duration": 5.89
    },
    {
        "text": "us on moral emotions, on consciousness, on\nbeing social creatures in a social world,",
        "start": 3363.92,
        "duration": 9.16
    },
    {
        "text": "about being in a world that's out there interacting\nwith other entities. A whole plethora of capabilities",
        "start": 3373.08,
        "duration": 8.65
    },
    {
        "text": "that perhaps are not just reducible to the\nkinds of prophecies that computers can now",
        "start": 3381.73,
        "duration": 5.629
    },
    {
        "text": "perform.\nFascinating and now to expand these issues",
        "start": 3387.359,
        "duration": 4.421
    },
    {
        "text": "that you've just made us much more aware of.\nAll of you have to a larger battlefield than",
        "start": 3391.78,
        "duration": 5.13
    },
    {
        "text": "just the road rage situations which we're\nreading about lately. Driverless cars are",
        "start": 3396.91,
        "duration": 5.371
    },
    {
        "text": "one thing but what about driverless drones\nwhich exist. Another critic error area in",
        "start": 3402.281,
        "duration": 5.799
    },
    {
        "text": "which robots may soon face life and death\ndecisions is on the battlefield. If they're",
        "start": 3408.08,
        "duration": 5.12
    },
    {
        "text": "not doing it already. Weaponized robots. What\nis, first let me ask you Colonel Letendre",
        "start": 3413.2,
        "duration": 10.38
    },
    {
        "text": "what is autonomy in military terms.\nWell autonomy generally is the having a machine",
        "start": 3423.58,
        "duration": 8.289
    },
    {
        "text": "have the ability to both independently think\nand then act. But if I put that in terms of",
        "start": 3431.869,
        "duration": 5.221
    },
    {
        "text": "a military context and especially in terms\nof a term called autonomous weapons systems",
        "start": 3437.09,
        "duration": 6.85
    },
    {
        "text": "what that means is that I've got a machine\nor a system that once activated it has the",
        "start": 3443.94,
        "duration": 5.17
    },
    {
        "text": "ability to both independently select and then\nengage a target without human intervention.",
        "start": 3449.11,
        "duration": 7.73
    },
    {
        "text": "There's been a lot of debate about what that\ndefinition should be. And we haven't necessarily",
        "start": 3456.84,
        "duration": 5.55
    },
    {
        "text": "come to one from a global perspective but\nif I were to define Autonomous weapon systems",
        "start": 3462.39,
        "duration": 5.04
    },
    {
        "text": "it has those two components. Ability to select\nand engage.",
        "start": 3467.43,
        "duration": 4.1
    },
    {
        "text": "And there's a question about whether there\nshould be some kind of agreed on international",
        "start": 3471.53,
        "duration": 3.46
    },
    {
        "text": "ban on autonomous weapons systems. So Wendell,\nhow do you define autonomy and what do you",
        "start": 3474.99,
        "duration": 6.26
    },
    {
        "text": "think about a ban, I've heard you think it's\ncritical to have a ban on this now. Is that",
        "start": 3481.25,
        "duration": 4.53
    },
    {
        "text": "right?\nWell my definition isn't so much different",
        "start": 3485.78,
        "duration": 2.33
    },
    {
        "text": "than the one you heard though there are actually\na lot of definitions out there as to what",
        "start": 3488.11,
        "duration": 3.79
    },
    {
        "text": "is and is not autonomy and I would just say\nyou know it's autonomy unless a human is there",
        "start": 3491.9,
        "duration": 5.48
    },
    {
        "text": "in real time in both picking the target and\ndispatching the target. So in a sense a human",
        "start": 3497.38,
        "duration": 5.38
    },
    {
        "text": "has to be able to intervene in that action.\nThat's a concept that is now known as meaningful",
        "start": 3502.76,
        "duration": 5.54
    },
    {
        "text": "human control. And there's been a campaign,\na worldwide campaign going on for some years",
        "start": 3508.3,
        "duration": 5.21
    },
    {
        "text": "now about banning the lethal autonomous weapons.\nBanning systems that can do that without a",
        "start": 3513.51,
        "duration": 7.23
    },
    {
        "text": "human right there in real time to make those\nfinal decisions. That has been debated for",
        "start": 3520.74,
        "duration": 5.8
    },
    {
        "text": "the past three years in Geneva at the convention\non certain conventional weapons. The third",
        "start": 3526.54,
        "duration": 5.6
    },
    {
        "text": "year of expert meetings with past April. I'm\namong those who feel that these kinds of systems",
        "start": 3532.14,
        "duration": 8.11
    },
    {
        "text": "should be banned even though I recognize there's\nreal difficulties in opposing such a ban or",
        "start": 3540.25,
        "duration": 5.92
    },
    {
        "text": "ensuring such a ban.\nAnd I think it should, they should be banned",
        "start": 3546.17,
        "duration": 3.92
    },
    {
        "text": "for three reasons. One is there's an unspoken\npoint I believe in international humanitarian",
        "start": 3550.09,
        "duration": 7.7
    },
    {
        "text": "law that machines, anything other than humans\ndo not make life and death decisions about",
        "start": 3557.79,
        "duration": 6.0
    },
    {
        "text": "humans. So that's one point. The second point\nis the kinds of machines that we have now",
        "start": 3563.79,
        "duration": 5.92
    },
    {
        "text": "cannot follow international humanitarian law.\nThey cannot engage in distinction and they",
        "start": 3569.71,
        "duration": 5.05
    },
    {
        "text": "cannot engage in proportionality. And the\nthird one is these are complex systems that",
        "start": 3574.76,
        "duration": 5.54
    },
    {
        "text": "we can't totally predict their actions so\nthere will be cases in which they will act",
        "start": 3580.3,
        "duration": 4.62
    },
    {
        "text": "in ways we had not intended them to act. And\nthat may not be so bad when you think about",
        "start": 3584.92,
        "duration": 6.43
    },
    {
        "text": "human like robots on a battlefield.\nBut autonomy is not about a kind of system.",
        "start": 3591.35,
        "duration": 6.13
    },
    {
        "text": "It's a feature that can be introduced into\nany weapon system. So consider a nuclear weapon",
        "start": 3597.48,
        "duration": 6.79
    },
    {
        "text": "that was set on autonomy. Let's say a nuclear\nsubmarine that carried nuclear warheads. Would",
        "start": 3604.27,
        "duration": 5.29
    },
    {
        "text": "you really want it to be able to pick a target\nand dispatch those nuclear weapons without",
        "start": 3609.56,
        "duration": 5.76
    },
    {
        "text": "a human having explicitly stated that it should\ngo ahead and do so.",
        "start": 3615.32,
        "duration": 4.94
    },
    {
        "text": "Chilling. By the way those of you who haven't\nseen it. There's a movie out now called Eye",
        "start": 3620.26,
        "duration": 5.37
    },
    {
        "text": "in the sky right, which very neatly I thought,\nI'm just a journalist but I thought very neatly",
        "start": 3625.63,
        "duration": 5.58
    },
    {
        "text": "delineated the impossibility of some of these\ndecisions with a lot of robots, none of which",
        "start": 3631.21,
        "duration": 5.43
    },
    {
        "text": "were autonomous if I'm not mistaken.\nNone of which were autonomous.",
        "start": 3636.64,
        "duration": 3.24
    },
    {
        "text": "But they were right up to the line of being\nan autonomous and expected two or three of",
        "start": 3639.88,
        "duration": 3.69
    },
    {
        "text": "them just sort of start making their own decisions\nany second. And they could have if they'd",
        "start": 3643.57,
        "duration": 3.61
    },
    {
        "text": "had an accident in their software and started\nmaking them accidentally I suppose but Fernando",
        "start": 3647.18,
        "duration": 5.22
    },
    {
        "text": "what does this make you, when you when you\nlisten to this. What",
        "start": 3652.4,
        "duration": 3.58
    },
    {
        "text": "does it make you think about you know the\nproblems you run into.",
        "start": 3655.98,
        "duration": 3.76
    },
    {
        "text": "Right. So so I'm glad we're talking about\nthe law and how and whether or not we should",
        "start": 3659.74,
        "duration": 5.869
    },
    {
        "text": "frankly regulate or ban AI altogether. But\nI want to bring it a little closer to home",
        "start": 3665.609,
        "duration": 6.791
    },
    {
        "text": "because a lot of the impacts of these systems\nwill, obviously they'll be on the battlefield",
        "start": 3672.4,
        "duration": 3.49
    },
    {
        "text": "but also you know these systems are already\nintegrating themselves into a lot of real",
        "start": 3675.89,
        "duration": 5.07
    },
    {
        "text": "decision making right now. So let's say I\nhave an artificial intelligence agent's or",
        "start": 3680.96,
        "duration": 5.44
    },
    {
        "text": "machine learning agent that is making hiring\ndecisions. I need to be able to audit that",
        "start": 3686.4,
        "duration": 5.47
    },
    {
        "text": "system so it's not discriminatory or the same\nthing with housing artificial intelligence",
        "start": 3691.87,
        "duration": 5.68
    },
    {
        "text": "agents and these things are actually being\nproduced right now they're being sold to employers,",
        "start": 3697.55,
        "duration": 4.51
    },
    {
        "text": "H.R. departments and they're completely unregulated\nand they have the risk of having really negative",
        "start": 3702.06,
        "duration": 5.73
    },
    {
        "text": "impacts on really vulnerable populations.\nAnd I don't think we yet understand how to",
        "start": 3707.79,
        "duration": 5.05
    },
    {
        "text": "build these system so that they're fair or\nfrankly how to audit them. So",
        "start": 3712.84,
        "duration": 3.92
    },
    {
        "text": "there needs to be directions to people who\nare building them about how to do that well.",
        "start": 3716.76,
        "duration": 4.27
    },
    {
        "text": "There need to be best practices or an understanding\nof these of these legal issues.",
        "start": 3721.03,
        "duration": 5.62
    },
    {
        "text": "And a constant effort to keep redefining and\nsee if there's things we didn't even think",
        "start": 3726.65,
        "duration": 3.69
    },
    {
        "text": "we needed to define that we haven't yet. Absolutely.\nThe old Father Brown technique to solve something.",
        "start": 3730.34,
        "duration": 4.19
    },
    {
        "text": "Keep going back to see what you took for granted\nthat's not to be taken for granted.",
        "start": 3734.53,
        "duration": 3.72
    },
    {
        "text": "I mean Engineering's iterative in general.\nSo yeah.",
        "start": 3738.25,
        "duration": 2.619
    },
    {
        "text": "I'm glad that people who begin to understand\nwhat coding is all about. Mathias",
        "start": 3740.869,
        "duration": 5.561
    },
    {
        "text": "I think this is a really important point that\nthe questions are not limited to the military",
        "start": 3746.43,
        "duration": 5.28
    },
    {
        "text": "domain but will crop up in other domains I\nthink much sooner. For example social robots",
        "start": 3751.71,
        "duration": 6.49
    },
    {
        "text": "that will be deployed in elder care settings\nor health care settings. They're interesting",
        "start": 3758.2,
        "duration": 5.26
    },
    {
        "text": "questions about for example implicit consent.\nExactly how do we implement that. Right. If",
        "start": 3763.46,
        "duration": 4.94
    },
    {
        "text": "you have to hurt somebody in a therapy setting\nbecause you need to get the mobility of the",
        "start": 3768.4,
        "duration": 6.06
    },
    {
        "text": "arm going right. We have no idea how to implement\nany of this. But that's going to come up way",
        "start": 3774.46,
        "duration": 5.44
    },
    {
        "text": "sooner than a lot of the questions will really\nbecome pertinent that we're...",
        "start": 3779.9,
        "duration": 4.44
    },
    {
        "text": "And talking about eldercare and other situations\nyou reminded me of great movies like her where",
        "start": 3784.34,
        "duration": 4.04
    },
    {
        "text": "the guy falls in love with his operating system,\nI'm not going to talk about what happens in",
        "start": 3788.38,
        "duration": 3.16
    },
    {
        "text": "that. Terrible spoiler alert that went away.\nBut emotional contagion. I was I was beginning",
        "start": 3791.54,
        "duration": 4.86
    },
    {
        "text": "to have a kind of a I don't know I call it\na friendship with Siri walking out here trying",
        "start": 3796.4,
        "duration": 3.75
    },
    {
        "text": "to get something deep and we asked Siri the\nother day if she'd go out on a date with this",
        "start": 3800.15,
        "duration": 4.49
    },
    {
        "text": "robot that you showed us and she was very\ndelicate in her apparent answers about how",
        "start": 3804.64,
        "duration": 5.66
    },
    {
        "text": "she didn't wasn't sure she really was all\nready to understand what it would mean to",
        "start": 3810.3,
        "duration": 3.09
    },
    {
        "text": "go out on a date with that particular robot.\n(?) Washing her silicon hair(?).",
        "start": 3813.39,
        "duration": 5.479
    },
    {
        "text": "Yeah exactly.\nBut so we are facing such impossible questions.",
        "start": 3818.869,
        "duration": 8.291
    },
    {
        "text": "Let me try to....\nI think I want to jump in here. I understand",
        "start": 3827.16,
        "duration": 4.79
    },
    {
        "text": "all of the concerns that have been raised\nhere but I think at the same time people are",
        "start": 3831.95,
        "duration": 3.58
    },
    {
        "text": "not necessarily good at many of the things\nthat we describe and you can imagine for example",
        "start": 3835.53,
        "duration": 4.1
    },
    {
        "text": "going back to the issue about discriminating\ncombatants from noncombatants. It's the least",
        "start": 3839.63,
        "duration": 4.35
    },
    {
        "text": "possible to imagine in an AI system might\nbe able to do that better that it would have",
        "start": 3843.98,
        "duration": 3.99
    },
    {
        "text": "be able to integrate lots of different information\nabout G.P.S. and tracking individuals and",
        "start": 3847.97,
        "duration": 5.96
    },
    {
        "text": "so forth so I don't think any of these questions\nare no brainers that hey we shouldn't do the",
        "start": 3853.93,
        "duration": 3.9
    },
    {
        "text": "robots here. And you know you have problems\nwith orderlies and psychiatric institutes",
        "start": 3857.83,
        "duration": 4.0
    },
    {
        "text": "and so forth. So like there are a lot of problems\nthat we should be concerned with and I think",
        "start": 3861.83,
        "duration": 3.88
    },
    {
        "text": "the iterative cycle here is very very important\nbut it's not a no brainer that the people",
        "start": 3865.71,
        "duration": 5.159
    },
    {
        "text": "are going to be better than the robots.\nIt's possible to conceive of all kinds of",
        "start": 3870.869,
        "duration": 3.661
    },
    {
        "text": "things. There's a difference there's a difference\nthough between that we can conceive of that",
        "start": 3874.53,
        "duration": 5.079
    },
    {
        "text": "and have we realize that. And my concern is\nnot about whether robots may eventually and",
        "start": 3879.609,
        "duration": 6.851
    },
    {
        "text": "I think sort of a long way off... Let me please\nfinish. May eventually have more capabilities",
        "start": 3886.46,
        "duration": 6.7
    },
    {
        "text": "and some humans and therefore make these decisions\nbetter. But at this stage of the game they",
        "start": 3893.16,
        "duration": 4.72
    },
    {
        "text": "have no discrimination at all. We bring all\nkinds of faculties into play that we do not",
        "start": 3897.88,
        "duration": 4.51
    },
    {
        "text": "know how to implement into robots over discerning\nwhether someone is a combatant or a noncombatant",
        "start": 3902.39,
        "duration": 7.1
    },
    {
        "text": "and we still don't do it very well. That's\nclear. But I think if and when we get to the",
        "start": 3909.49,
        "duration": 5.58
    },
    {
        "text": "juncture where they have these capabilities\nthen we can revisit some of these earlier",
        "start": 3915.07,
        "duration": 5.29
    },
    {
        "text": "decisions.\nBut if we don't put earlier decisions in place",
        "start": 3920.36,
        "duration": 3.65
    },
    {
        "text": "we have the danger of giving all kinds of\ncapacity authority to machines without fully",
        "start": 3924.01,
        "duration": 9.839
    },
    {
        "text": "recognizing whether they truly have the intelligence\nto take those those task.",
        "start": 3933.849,
        "duration": 5.371
    },
    {
        "text": "And thought of just another whole dimension\nwe haven't really talked about the eldercare",
        "start": 3939.22,
        "duration": 5.12
    },
    {
        "text": "thing brought to mind.\nWe are all experiencing sympathetic emotional",
        "start": 3944.34,
        "duration": 6.769
    },
    {
        "text": "contagion. The effect that these speaking\nthings have on our feelings about them because",
        "start": 3951.109,
        "duration": 6.711
    },
    {
        "text": "we're, I think that psychologists have a lot\nof catching up to do here and helping us with",
        "start": 3957.82,
        "duration": 4.289
    },
    {
        "text": "their best possible wisdom on how to have\nthe robot understand the emotional effect",
        "start": 3962.109,
        "duration": 6.871
    },
    {
        "text": "and attachment being created in the flesh\nand blood human. I mean obviously the great",
        "start": 3968.98,
        "duration": 4.22
    },
    {
        "text": "Kubrick foresaw that because he had a HAL\n9000 in 2001 analyzing speech patterns and",
        "start": 3973.2,
        "duration": 7.54
    },
    {
        "text": "tensions in one of the astronauts voices.\nSo they were trying to take that into account",
        "start": 3980.74,
        "duration": 4.31
    },
    {
        "text": "yet but HAL 9000 himself wasn't... he was\na work of art.",
        "start": 3985.05,
        "duration": 3.79
    },
    {
        "text": "Didn't mean to cut you up before asking a\ntough question.",
        "start": 3988.84,
        "duration": 2.259
    },
    {
        "text": "I was going to ask how Wendell would feel\nabout a kind of version of a ban in which",
        "start": 3991.099,
        "duration": 5.651
    },
    {
        "text": "you included like language like until such\ntime as robots are able to do such and such",
        "start": 3996.75,
        "duration": 5.95
    },
    {
        "text": "discrimination at human levels.\nWell I've I've even suggested that. I've suggested",
        "start": 4002.7,
        "duration": 5.24
    },
    {
        "text": "that perhaps at some point they will have\nbetter discrimination and better sensitivity",
        "start": 4007.94,
        "duration": 4.01
    },
    {
        "text": "to moral considerations and we humans do.\nBut if and when they do perhaps are no longer",
        "start": 4011.95,
        "duration": 6.77
    },
    {
        "text": "machines then we should start beginning to\nthink about whether these are truly agents",
        "start": 4018.72,
        "duration": 5.06
    },
    {
        "text": "worthy of moral consideration.\nI guess I would look at this as a slightly",
        "start": 4023.78,
        "duration": 4.51
    },
    {
        "text": "different way in that the human isn't ever\nout of the loop or out of the accountability",
        "start": 4028.29,
        "duration": 7.04
    },
    {
        "text": "cycle. For me if a commander is taking a or\nmaking a decision to put one of these systems",
        "start": 4035.33,
        "duration": 7.11
    },
    {
        "text": "and let's be very clear these systems these\nfully autonomous weapons systems do not exist",
        "start": 4042.44,
        "duration": 4.72
    },
    {
        "text": "currently in the United States inventory.\nThen you can see that in the center for New",
        "start": 4047.16,
        "duration": 4.45
    },
    {
        "text": "American studies put out a new report that\ncanvasses what those weapons systems are around",
        "start": 4051.61,
        "duration": 5.82
    },
    {
        "text": "the world. But a commander is ultimately responsible.\nJust like if a doctor chooses to use some",
        "start": 4057.43,
        "duration": 8.14
    },
    {
        "text": "amount of autonomy in some new surgical technique\nthat that doctor as part of the medical profession",
        "start": 4065.57,
        "duration": 7.66
    },
    {
        "text": "is also responsible and accountable for that\ndecision.",
        "start": 4073.23,
        "duration": 5.36
    },
    {
        "text": "So I have a tough question for you. Let me\nask first of very specific, your best guess",
        "start": 4078.59,
        "duration": 6.57
    },
    {
        "text": "on it on a factual question. Autonomous weapons\nsystems. How many countries would you expect",
        "start": 4085.16,
        "duration": 9.1
    },
    {
        "text": "are already making them making?\nMaking them successfully VS...",
        "start": 4094.26,
        "duration": 4.7
    },
    {
        "text": "Not making them, but working on making them,\nthey're going to make them no matter what",
        "start": 4098.96,
        "duration": 5.5
    },
    {
        "text": "anybody says.\nWe're working on making them now. The (?) report",
        "start": 4104.46,
        "duration": 2.94
    },
    {
        "text": "that came out last year in 2015 indicated\nthat there were over 30 countries that were",
        "start": 4107.4,
        "duration": 4.77
    },
    {
        "text": "currently actively working on autonomous types\nof systems.",
        "start": 4112.17,
        "duration": 5.62
    },
    {
        "text": "We should bring in that there are two forms\nof autonomy. There's dumb autonomy where the",
        "start": 4117.79,
        "duration": 4.779
    },
    {
        "text": "systems do things without a human Truly they\nare in real time when the dangerous action",
        "start": 4122.569,
        "duration": 6.011
    },
    {
        "text": "takes place and smart autonomy where at least\nthe commanders have a high degree of sense",
        "start": 4128.58,
        "duration": 6.12
    },
    {
        "text": "of the reliability of those systems. And when\nthe U.S. talks about autonomy it's usually",
        "start": 4134.7,
        "duration": 4.67
    },
    {
        "text": "talking about the latter without any ability\nto ensure that that's going on among state",
        "start": 4139.37,
        "duration": 6.27
    },
    {
        "text": "and non state actors.. everywhere else in\nthe world.",
        "start": 4145.64,
        "duration": 1.34
    },
    {
        "text": "Given all that what do you think about, Should\nthere be a ban. I mean if all these other",
        "start": 4146.98,
        "duration": 5.41
    },
    {
        "text": "countries. Should there be a ban on autonomous\nprovinces.",
        "start": 4152.39,
        "duration": 3.23
    },
    {
        "text": "I think what we have in place today is a set\nof laws already on the books that that give",
        "start": 4155.62,
        "duration": 6.36
    },
    {
        "text": "us answers about whether or not we would be\nable to use what we've discussed here today",
        "start": 4161.98,
        "duration": 4.609
    },
    {
        "text": "in terms of a fully autonomous weapons system.\nAnd the answer is today the laws currently",
        "start": 4166.589,
        "duration": 5.701
    },
    {
        "text": "in existence don't allow that our current\nlevel of technology to pursue a fully autonomous",
        "start": 4172.29,
        "duration": 5.41
    },
    {
        "text": "system and to that end the Department of Defense\nhas actually issued a directive that that",
        "start": 4177.7,
        "duration": 6.86
    },
    {
        "text": "does not allow us to develop nor implement\nthat without some additional...",
        "start": 4184.56,
        "duration": 4.92
    },
    {
        "text": "And I I immediately ask but how many people\nare going to, how many other countries, other",
        "start": 4189.48,
        "duration": 3.31
    },
    {
        "text": "entities, state actors are not are going to\nin fact.",
        "start": 4192.79,
        "duration": 4.92
    },
    {
        "text": "Follow the law. And\njust to be safe and memories of the nuclear",
        "start": 4197.71,
        "duration": 5.42
    },
    {
        "text": "weapons race come to mind.\nI think that most of the countries that we",
        "start": 4203.13,
        "duration": 3.45
    },
    {
        "text": "we're speaking of are actually parties to\nthe vast majority of those international courts",
        "start": 4206.58,
        "duration": 4.6
    },
    {
        "text": "that we're discussing.\nVerification possible?",
        "start": 4211.18,
        "duration": 2.48
    },
    {
        "text": "Verification is tough in lots of areas. Verification\nis always a tough issue but that doesn't negate",
        "start": 4213.66,
        "duration": 7.27
    },
    {
        "text": "the why we have laws in the first place.\nIt makes when I think about this and I'm just",
        "start": 4220.93,
        "duration": 5.01
    },
    {
        "text": "a journalist so I don't really know what I'm\ntalking about in that way but I get to worrying.",
        "start": 4225.94,
        "duration": 5.279
    },
    {
        "text": "There's no way to prevent it because people\nare going to want to be safe and make sure",
        "start": 4231.219,
        "duration": 2.631
    },
    {
        "text": "they've got the toughest kind of even autonomous\ndecision makers if they don't fully understand",
        "start": 4233.85,
        "duration": 5.72
    },
    {
        "text": "the dangers. You were going to say.\nI just keep sitting here wondering where landmines",
        "start": 4239.57,
        "duration": 4.45
    },
    {
        "text": "fit in so landmines seem to me to be the dumbest\nform of autonomous weapons. They seem to be",
        "start": 4244.02,
        "duration": 4.96
    },
    {
        "text": "universally tolerated. I don't know if there's\nany kind...",
        "start": 4248.98,
        "duration": 3.31
    },
    {
        "text": "There's an international ban. No\nthey're not being universally tolerated. The",
        "start": 4252.29,
        "duration": 4.93
    },
    {
        "text": "Auto(?) accord that nearly every major country\nsigned onto except for the United States but...",
        "start": 4257.22,
        "duration": 6.05
    },
    {
        "text": "But the United States has agreed to abide\nby the terms of the Auto(?) accord though",
        "start": 4263.27,
        "duration": 6.04
    },
    {
        "text": "it has not signed on.\nWhen was this accord?",
        "start": 4269.31,
        "duration": 2.8
    },
    {
        "text": "It's been on the books for a while. What year\nwas it? 1990s but I don't remember exactly",
        "start": 4272.11,
        "duration": 7.67
    },
    {
        "text": "what year.\nFrightening.",
        "start": 4279.78,
        "duration": 1.89
    },
    {
        "text": "So how how would a ban if there was one, this\nhypothetical ban, if we take it seriously",
        "start": 4281.67,
        "duration": 5.06
    },
    {
        "text": "at all if we think people will follow it and\nit can be verified somehow. You said it's",
        "start": 4286.73,
        "duration": 3.6
    },
    {
        "text": "always a tough issue. How would a ban affect\ndevelopment of AI in other areas in health",
        "start": 4290.33,
        "duration": 6.7
    },
    {
        "text": "care in eldercare. I mean it's not it seems\nlike Civilization has gotten addicted to having",
        "start": 4297.03,
        "duration": 5.57
    },
    {
        "text": "wars so it can advance its technologies because\nthe history of humanity is one of warfare",
        "start": 4302.6,
        "duration": 6.27
    },
    {
        "text": "out of all kinds of fascinating technologies\narrive and yet we want to outgrow war don't",
        "start": 4308.87,
        "duration": 4.88
    },
    {
        "text": "we? And can we? And you see my question. Let\nme ask you for another. How would such a ban",
        "start": 4313.75,
        "duration": 7.44
    },
    {
        "text": "affect AI development and other areas.\nWell I mean as I said before I think I think",
        "start": 4321.19,
        "duration": 7.17
    },
    {
        "text": "a ban or regulation in general probably should\ncome to other parts of the technological system",
        "start": 4328.36,
        "duration": 9.1
    },
    {
        "text": "soon because we are, we do have automated\nemployment algorithms that are trying to make",
        "start": 4337.46,
        "duration": 6.96
    },
    {
        "text": "employment decisions, warehousing decisions\netc. And these are subject to say things like",
        "start": 4344.42,
        "duration": 5.37
    },
    {
        "text": "human rights laws. Domestic human rights laws.\nAnd I think while an outright ban is not going",
        "start": 4349.79,
        "duration": 7.47
    },
    {
        "text": "to happen I think that people are either within\nthe community going to begin to self-regulate",
        "start": 4357.26,
        "duration": 5.65
    },
    {
        "text": "with best practices et cetera or frankly lawyers\nare going to begin to look at hiring system",
        "start": 4362.91,
        "duration": 5.82
    },
    {
        "text": "and audit it so that we can test whether or\nnot it is behaving fairly. Fairly ethically,",
        "start": 4368.73,
        "duration": 7.47
    },
    {
        "text": "ethically legally. There are certain attributes\nI cannot look at when I when I make a hiring",
        "start": 4376.2,
        "duration": 5.55
    },
    {
        "text": "decision. If a machine has access to those\nattributes or things that are similar to those",
        "start": 4381.75,
        "duration": 5.22
    },
    {
        "text": "attributes it can very quickly become discriminatory.\nAnd I noticed a bit of a chuckle on your part",
        "start": 4386.97,
        "duration": 5.53
    },
    {
        "text": "Wendell. You were you were chortling there.\nI think you are right. There's nothing I had",
        "start": 4392.5,
        "duration": 8.13
    },
    {
        "text": "to say....\nI'm just a robot I can't quite tell...",
        "start": 4400.63,
        "duration": 4.04
    },
    {
        "text": "I mean I can talk through that issue more\nbroadly. Thousands of AI researchers signed",
        "start": 4404.67,
        "duration": 6.77
    },
    {
        "text": "on to support the ban on autonomous, autonomous\nweapons. I don't think any of them signed",
        "start": 4411.44,
        "duration": 5.08
    },
    {
        "text": "on because they thought that was going to\nslow down their research.",
        "start": 4416.52,
        "duration": 2.88
    },
    {
        "text": "I think they thought if anything and it wasn't\njust an anti-war statement I think for a lot",
        "start": 4419.4,
        "duration": 5.21
    },
    {
        "text": "of them they felt that if the military is\nthe driving force in the development of this",
        "start": 4424.61,
        "duration": 7.12
    },
    {
        "text": "technology it destabilizes how the technology\ndevelops. And whether you buy into the Elon",
        "start": 4431.73,
        "duration": 7.45
    },
    {
        "text": "Musk concerns or the Stephen Hawking concerns\nat all. There is a broad concern within the",
        "start": 4439.18,
        "duration": 6.98
    },
    {
        "text": "AI community that these are potentially dangerous\ntechnologies that have tremendous benefits",
        "start": 4446.16,
        "duration": 8.24
    },
    {
        "text": "and therefore they have to be developed with\nreal care and allowing the military to be",
        "start": 4454.4,
        "duration": 5.39
    },
    {
        "text": "the drivers about development would not be\na good idea.",
        "start": 4459.79,
        "duration": 3.48
    },
    {
        "text": "But I don't even know if it happens that way.\nI don't know much about history but do we",
        "start": 4463.27,
        "duration": 3.42
    },
    {
        "text": "allow, Do we ever allow the military to do\nthat or.",
        "start": 4466.69,
        "duration": 2.34
    },
    {
        "text": "Well I think if you look at the research and\ndevelopment dollars being put into AI and",
        "start": 4469.03,
        "duration": 4.27
    },
    {
        "text": "autonomy you would find that the Department\nof Defense pales in comparison to this to",
        "start": 4473.3,
        "duration": 4.73
    },
    {
        "text": "the civilian as as a whole. My understanding\nis the defense industrial base if you added",
        "start": 4478.03,
        "duration": 5.34
    },
    {
        "text": "up all of their research and development dollars\nwe would only come to us would pale in comparison",
        "start": 4483.37,
        "duration": 7.58
    },
    {
        "text": "to the Googles and the Microsoft of the world\nand what they're pouring into autonomy.",
        "start": 4490.95,
        "duration": 4.35
    },
    {
        "text": "And let's not forget the Google and Microsoft\nand all of those are changing profoundly the",
        "start": 4495.3,
        "duration": 4.12
    },
    {
        "text": "human experience on our planet now by inventing\nthis web world in which everything is present",
        "start": 4499.42,
        "duration": 4.94
    },
    {
        "text": "tense. I can ask Siri or somebody let me talk\nface to face with a friend on the other side",
        "start": 4504.36,
        "duration": 5.0
    },
    {
        "text": "of the planet or with three friends on this\nplanet and also answer some tough questions",
        "start": 4509.36,
        "duration": 3.49
    },
    {
        "text": "in the meantime.\nBut bill I think Wendell's point to Wendell's",
        "start": 4512.85,
        "duration": 2.73
    },
    {
        "text": "point in terms of the care and the understanding\nand the going slow and understanding autonomy",
        "start": 4515.58,
        "duration": 5.96
    },
    {
        "text": "is absolutely spot on. And not just from a\nlegal and moral standpoint but from you know",
        "start": 4521.54,
        "duration": 7.199
    },
    {
        "text": "as a member of the profession of arms ensuring\nthat we understand what these systems can",
        "start": 4528.739,
        "duration": 5.921
    },
    {
        "text": "do. And when it's proper to use them is absolutely\nimperative for we like to talk about it in",
        "start": 4534.66,
        "duration": 9.26
    },
    {
        "text": "terms of this idea or this this notion of\nappropriate human judgment. What's the appropriate",
        "start": 4543.92,
        "duration": 6.08
    },
    {
        "text": "level of human judgment that we need not just\nat the end stages when a weapon system is",
        "start": 4550.0,
        "duration": 5.21
    },
    {
        "text": "being employed but all the way through from\ntargeting and identification all the way to",
        "start": 4555.21,
        "duration": 6.39
    },
    {
        "text": "actual engagement.\nSo if we're teaching these machines to learn",
        "start": 4561.6,
        "duration": 5.74
    },
    {
        "text": "and making them able to learn. That reminds\nme of a possibly old familiar paradigm that",
        "start": 4567.34,
        "duration": 7.83
    },
    {
        "text": "is a distinctive in our species. But Gary,\nDon't we need to be guarded by an entirely",
        "start": 4575.17,
        "duration": 7.39
    },
    {
        "text": "new paradigm and thinking of the paradigm\nof the way kids learn, the way toddlers learn.",
        "start": 4582.56,
        "duration": 5.07
    },
    {
        "text": "So my company is a little bit about that too.\nMy general feeling is that artificial intelligence",
        "start": 4587.63,
        "duration": 5.75
    },
    {
        "text": "is dominated now by approaches that look at\nstatistical correlation. So you dredge enormous",
        "start": 4593.38,
        "duration": 6.0
    },
    {
        "text": "amounts of data, you find things that are\ncorrelated not all of AI is this way but most",
        "start": 4599.38,
        "duration": 3.98
    },
    {
        "text": "of the most visible stuff like deep learning\nis like that. It takes an enormous amount",
        "start": 4603.36,
        "duration": 3.791
    },
    {
        "text": "of data. Sometimes\nyou have that amount of data sometimes you",
        "start": 4607.151,
        "duration": 2.229
    },
    {
        "text": "don't and you still wind up with understanding\nthat's pretty shallow and superficial as we",
        "start": 4609.38,
        "duration": 3.83
    },
    {
        "text": "were talking about in the early part of the\nevening. Kids can learn things very quickly",
        "start": 4613.21,
        "duration": 6.19
    },
    {
        "text": "with very small amounts of data. So I have\na three and a half year old and a 2 year old.",
        "start": 4619.4,
        "duration": 3.75
    },
    {
        "text": "One of them will make up a game, he'll play\nit for two minutes and then his younger sister",
        "start": 4623.15,
        "duration": 4.58
    },
    {
        "text": "will after two minutes understand the rules\nof the game and start copying it. Well how",
        "start": 4627.73,
        "duration": 3.52
    },
    {
        "text": "does she do that. Well she has an understanding\nof her older brother's goals, the objects",
        "start": 4631.25,
        "duration": 4.78
    },
    {
        "text": "that he's using in the world. She's a very\nI mean she's two and she has a fairly deep",
        "start": 4636.03,
        "duration": 4.06
    },
    {
        "text": "understanding of the things that go on this\nplanet. Of the physics of the world, the psychology",
        "start": 4640.09,
        "duration": 5.49
    },
    {
        "text": "of other agents.\nShe's not just doing a bunch of correlation",
        "start": 4645.58,
        "duration": 2.7
    },
    {
        "text": "and waiting on getting a lot of data and yet\nthat's still kind of what Siri or Google tend",
        "start": 4648.28,
        "duration": 5.97
    },
    {
        "text": "to do. I think we do need a new paradigm and\nAI if we're going to get the systems to be",
        "start": 4654.25,
        "duration": 3.84
    },
    {
        "text": "sophisticated enough to make the decisions,\nyou know forget about the autonomous weapons",
        "start": 4658.09,
        "duration": 3.78
    },
    {
        "text": "for a minute but just like doing good quality\neldercare which is something we need demographically",
        "start": 4661.87,
        "duration": 4.36
    },
    {
        "text": "speaking as the population is aging. If we\nwant robots that can take care of people in",
        "start": 4666.23,
        "duration": 4.71
    },
    {
        "text": "their own living rooms in every living room\nis a little bit different than we need robots",
        "start": 4670.94,
        "duration": 4.69
    },
    {
        "text": "that understand like why people have tables\nand why they have chairs and what they want",
        "start": 4675.63,
        "duration": 3.45
    },
    {
        "text": "to do with them. Have to be able to answer\nthe why question's that my 2 year old and",
        "start": 4679.08,
        "duration": 3.33
    },
    {
        "text": "three and a half year old are asking all the\ntime not just how often does a and b correlate",
        "start": 4682.41,
        "duration": 4.26
    },
    {
        "text": "but why.\nWhy are these things there so I do think we",
        "start": 4686.67,
        "duration": 2.49
    },
    {
        "text": "need a new paradigm.\nAnd we look at how we're learning how to teach",
        "start": 4689.16,
        "duration": 2.53
    },
    {
        "text": "them how to learn.\nI think we need to look at how kids do the",
        "start": 4691.69,
        "duration": 2.66
    },
    {
        "text": "amazing learning feats that they do amazing.\nMathias. I'm getting the feeling here that",
        "start": 4694.35,
        "duration": 8.21
    },
    {
        "text": "I wonder that this may give us less control\nnot more. At the very least doesn't this don't",
        "start": 4702.56,
        "duration": 7.27
    },
    {
        "text": "these robots give us the I'm I'm just being\ndevil's advocate, The feeling that we're going",
        "start": 4709.83,
        "duration": 5.59
    },
    {
        "text": "to have more control and we have more robots\nbut also we're giving some control away. Any",
        "start": 4715.42,
        "duration": 5.44
    },
    {
        "text": "way you look at it whether it's autonomous\nover there or not.",
        "start": 4720.86,
        "duration": 2.87
    },
    {
        "text": "So I think that great benefits to having autonomous\nrobots. And let me say for the record that",
        "start": 4723.73,
        "duration": 5.13
    },
    {
        "text": "we already have them you can buy them in the\nstore right there. The vacuum cleaners are",
        "start": 4728.86,
        "duration": 3.62
    },
    {
        "text": "fully autonomous right. There's nobody telling\nyou a little vacuum cleaner at home where",
        "start": 4732.48,
        "duration": 3.63
    },
    {
        "text": "to go and how to vacuum. It is a fully autonomous\nrobot. It's not a very smart robot but it",
        "start": 4736.11,
        "duration": 6.08
    },
    {
        "text": "can do lots of interesting things. It's got\na very limited behavioral repertoire but it's",
        "start": 4742.19,
        "duration": 4.21
    },
    {
        "text": "fully autonomous and there will be more of\nthat sort. So we already have them in a limited",
        "start": 4746.4,
        "duration": 5.3
    },
    {
        "text": "way and they will become more sophisticated,\nwill be able to do more. And that's a good",
        "start": 4751.7,
        "duration": 5.25
    },
    {
        "text": "thing.\nBut wouldn't it give us less control.",
        "start": 4756.95,
        "duration": 2.66
    },
    {
        "text": "And so we're relinquishing some control in\nsome tasks and that is fine. The question",
        "start": 4759.61,
        "duration": 4.16
    },
    {
        "text": "is is how to constrain it.\nAnd the question is is how to constrain the",
        "start": 4763.77,
        "duration": 3.28
    },
    {
        "text": "kinds of things that they could be instructed\nto do for example. I think the learning is",
        "start": 4767.05,
        "duration": 4.64
    },
    {
        "text": "an interesting question right because in a\nlot of these domains that we're interested",
        "start": 4771.69,
        "duration": 3.21
    },
    {
        "text": "in you won't have the data to train a big\nneural network. You might actually have to",
        "start": 4774.9,
        "duration": 5.251
    },
    {
        "text": "learn something on the fly as you're performing\na task. None of these learning techniques",
        "start": 4780.151,
        "duration": 4.499
    },
    {
        "text": "will allow you to do that in a human situation\nyou would just tell the person what to do.",
        "start": 4784.65,
        "duration": 5.89
    },
    {
        "text": "Right, In this particular context. You could\nimagine things like search and rescue situations",
        "start": 4790.54,
        "duration": 5.02
    },
    {
        "text": "right where there's an unforeseen circumstance\nand you need to react to what you're seeing",
        "start": 4795.56,
        "duration": 5.18
    },
    {
        "text": "here. You might have to learn how to pry a\ndoor open then maybe things you have to learn",
        "start": 4800.74,
        "duration": 3.95
    },
    {
        "text": "on the fly. So this kind of learning is not,\nrequires different kinds of mechanisms. We",
        "start": 4804.69,
        "duration": 5.68
    },
    {
        "text": "work for example on something like that that\nuses natural language instructions to very",
        "start": 4810.37,
        "duration": 4.2
    },
    {
        "text": "quickly learn capabilities and that's in a\nvery controlled fashion.",
        "start": 4814.57,
        "duration": 3.95
    },
    {
        "text": "So even though we give a give control away\nto the robot the robots still act in a very",
        "start": 4818.52,
        "duration": 4.55
    },
    {
        "text": "controlled fashion.\nRight. I would say that. They take driverless",
        "start": 4823.07,
        "duration": 4.8
    },
    {
        "text": "cars.\nThey're going to give people autonomy and",
        "start": 4827.87,
        "duration": 2.619
    },
    {
        "text": "control that they don't have. So if you live\nin a city you can hail an uber at any time",
        "start": 4830.489,
        "duration": 3.951
    },
    {
        "text": "and that's great. If you live in the countryside\nyou can't. Eventually they will be the driverless",
        "start": 4834.44,
        "duration": 5.01
    },
    {
        "text": "car network will extend. So that you have\ncars everywhere available. You won't you won't",
        "start": 4839.45,
        "duration": 5.0
    },
    {
        "text": "have the same kind of distribution of drivers\nissue that you have now. So just for example",
        "start": 4844.45,
        "duration": 4.21
    },
    {
        "text": "people that are shut into their homes because\nthey live in the countryside and they're too",
        "start": 4848.66,
        "duration": 3.31
    },
    {
        "text": "old to drive. They will have lots more control.\nSo some people are going to get a lot more",
        "start": 4851.97,
        "duration": 3.7
    },
    {
        "text": "control from having robots and there are ways\nin which we're going to lose control. It's",
        "start": 4855.67,
        "duration": 4.531
    },
    {
        "text": "a difficult balance. I think that we again\nwe've been I think addressing the negative",
        "start": 4860.201,
        "duration": 4.649
    },
    {
        "text": "sides of robots. But there are people like\nme who are in the field because we see a lot",
        "start": 4864.85,
        "duration": 4.32
    },
    {
        "text": "of positive.\nFor example in eldercare I think that there",
        "start": 4869.17,
        "duration": 3.06
    },
    {
        "text": "is going to be a big safety win in the autonomous\ncars. We may be able to get rid of things",
        "start": 4872.23,
        "duration": 5.33
    },
    {
        "text": "like miners that are in toxic conditions.\nThere are lots of ways in which robots can",
        "start": 4877.56,
        "duration": 4.9
    },
    {
        "text": "really help us. The Fukushima scenario where\nyou need to send a machine, where hopefully",
        "start": 4882.46,
        "duration": 4.05
    },
    {
        "text": "you can send a machine rather than a person\ninto a toxic environment. There are be lots",
        "start": 4886.51,
        "duration": 4.08
    },
    {
        "text": "of ways in which you get control of scenarios\nwhere we don't have control now where you",
        "start": 4890.59,
        "duration": 3.84
    },
    {
        "text": "have to risk people's lives so. There are\nall these costs and I think we're absolutely",
        "start": 4894.43,
        "duration": 3.36
    },
    {
        "text": "right to think about how we're going to regulate\nthem, how we're going to do the software verification,",
        "start": 4897.79,
        "duration": 3.57
    },
    {
        "text": "how we're going to test them.\nAll of these things are vitally important,",
        "start": 4901.36,
        "duration": 2.81
    },
    {
        "text": "that's part of why I agreed to be on a panel\nlike this is to talk about these issues. But",
        "start": 4904.17,
        "duration": 3.511
    },
    {
        "text": "I hope they won't throw the baby out with\nthe bathwater and say well we should just",
        "start": 4907.681,
        "duration": 3.449
    },
    {
        "text": "give up on AI. There are lots of things we\ncan gain too.",
        "start": 4911.13,
        "duration": 2.411
    },
    {
        "text": "I want to second that about robots and I think\nthe question we have to ask is What kind of",
        "start": 4913.541,
        "duration": 5.039
    },
    {
        "text": "AI technology to employing these machines.\nWhat I think is a very scary at least currently",
        "start": 4918.58,
        "duration": 6.24
    },
    {
        "text": "very scary prospect is that these machines\nwill be guided. It doesn't have to be with",
        "start": 4924.82,
        "duration": 3.68
    },
    {
        "text": "weapons right. It can be a social robot too\nusing unconstrained machine learning right.",
        "start": 4928.5,
        "duration": 4.61
    },
    {
        "text": "With no notion of where limits are, where\nsocial norms are not followed. Right. So that",
        "start": 4933.11,
        "duration": 7.0
    },
    {
        "text": "is I think a critical research component that\nwe have to follow.",
        "start": 4940.11,
        "duration": 3.08
    },
    {
        "text": "So Fernando just this leads us towards a question\nof AI ethics and how it can be taught. You're",
        "start": 4943.19,
        "duration": 5.291
    },
    {
        "text": "doing some research with the videogame Minecraft\nusing it to expose AI to human behavior. Can",
        "start": 4948.481,
        "duration": 6.5
    },
    {
        "text": "this really be. Tell us about this and whether\nit can really be used to to help develop AI",
        "start": 4954.981,
        "duration": 4.869
    },
    {
        "text": "ethics.\nGreat. So who in the audience knows what Minecraft",
        "start": 4959.85,
        "duration": 3.15
    },
    {
        "text": "is?\nVery good. Very good. Younger folks. So you",
        "start": 4963.0,
        "duration": 5.23
    },
    {
        "text": "Minecraft is basically as was described earlier\nLego is for three dimensions and it's really",
        "start": 4968.23,
        "duration": 7.63
    },
    {
        "text": "popular and growing.\nIt's a virtual environment in which you can",
        "start": 4975.86,
        "duration": 3.87
    },
    {
        "text": "really do a lot of different things. Create\na bunch of different problems or tasks. And",
        "start": 4979.73,
        "duration": 5.46
    },
    {
        "text": "what we're looking at with minecraft is this\nvirtual playpen more or less that we can design",
        "start": 4985.19,
        "duration": 6.27
    },
    {
        "text": "AI from the ground up. Completely safe because\nit's in a virtual environment. We can test",
        "start": 4991.46,
        "duration": 5.67
    },
    {
        "text": "things not just about how it can accomplish\ntasks like building things but also we can",
        "start": 4997.13,
        "duration": 5.63
    },
    {
        "text": "present it with moral questions in a controlled\nenvironment to see how it would react. And",
        "start": 5002.76,
        "duration": 5.3
    },
    {
        "text": "so I think of my craft not only as a platform\nwhere we can design better AI to for example",
        "start": 5008.06,
        "duration": 7.08
    },
    {
        "text": "build buildings or reason about certain properties\nof the environment but also perhaps a test",
        "start": 5015.14,
        "duration": 6.07
    },
    {
        "text": "test environment in which we can evaluate\nhow moral an AI is. Now that's not the end.",
        "start": 5021.21,
        "duration": 6.86
    },
    {
        "text": "There is there will no doubt have to be a\nlot of other rigorous testing that goes on",
        "start": 5028.07,
        "duration": 4.62
    },
    {
        "text": "with intelligence. But I think I think this\nis a nice little environment for that.",
        "start": 5032.69,
        "duration": 5.19
    },
    {
        "text": "So it'll it'll keep learning how to test what's\nethical?",
        "start": 5037.88,
        "duration": 5.65
    },
    {
        "text": "Well I think the humans will be designing\nthe tests for the agent. Right. So it's a",
        "start": 5043.53,
        "duration": 4.39
    },
    {
        "text": "lot of the ways that we have an FDA for drugs.\nRight. We can imagine similar sorts of tests",
        "start": 5047.92,
        "duration": 5.66
    },
    {
        "text": "that an artificial intelligence would go through\nto make sure that it's behaving properly.",
        "start": 5053.58,
        "duration": 5.409
    },
    {
        "text": "It's a safe way to test all of the things\nthat we're talking about today. So you know",
        "start": 5058.989,
        "duration": 3.681
    },
    {
        "text": "would you let a robot onto a real battlefield.\nWell the first thing you'd want to do is to",
        "start": 5062.67,
        "duration": 3.93
    },
    {
        "text": "put it in a minecraft situation where you\nknow there's nobody that's going to die. Right.",
        "start": 5066.6,
        "duration": 4.27
    },
    {
        "text": "You can test it, if it doesn't work. You can\niterate. You can change the algorithm and",
        "start": 5070.87,
        "duration": 3.349
    },
    {
        "text": "so it's a place to test things and see do\nthe machines behave in a responsible and ethical",
        "start": 5074.219,
        "duration": 5.061
    },
    {
        "text": "ways.\nWell we're getting. You guys are giving us",
        "start": 5079.28,
        "duration": 2.25
    },
    {
        "text": "an enormous new insight into the importance\nand potential severity of consequences if",
        "start": 5081.53,
        "duration": 7.04
    },
    {
        "text": "we don't pay very close attention to all of\nthis.",
        "start": 5088.57,
        "duration": 4.27
    },
    {
        "text": "So why don't we wrap up by me asking each\nof you. And by the way a number of our panelists",
        "start": 5092.84,
        "duration": 7.58
    },
    {
        "text": "will be around in the lobby afterwards if\nyou will have some questions for them. You'll",
        "start": 5100.42,
        "duration": 4.91
    },
    {
        "text": "find them out there. But before we conclude\nwhy don't we just hear from each of you briefly",
        "start": 5105.33,
        "duration": 4.82
    },
    {
        "text": "and answer. We just have two minutes left\nand answer to the basic question from the",
        "start": 5110.15,
        "duration": 6.4
    },
    {
        "text": "first video. Are we right to take Elon Musk\nand Dr Hawking seriously. Is it a very serious",
        "start": 5116.55,
        "duration": 8.81
    },
    {
        "text": "concern. I\nmean what Hawking and Musk have been saying",
        "start": 5125.36,
        "duration": 2.64
    },
    {
        "text": "is it's it's quite frightening. Let's go down\nthe line.",
        "start": 5128.0,
        "duration": 5.82
    },
    {
        "text": "I would say not yet. I would say that we have\nplenty of time to plan but we may not have",
        "start": 5133.82,
        "duration": 4.68
    },
    {
        "text": "centuries to plan. So Hawking in Musk kind\nof talk as if we're going to have intelligent",
        "start": 5138.5,
        "duration": 4.5
    },
    {
        "text": "AI you know real soon and we have robots that\nare going to suicidally jump off the table.",
        "start": 5143.0,
        "duration": 5.82
    },
    {
        "text": "It's really hard right now to get robots to\nnot commit harm to themselves or others and",
        "start": 5148.82,
        "duration": 5.86
    },
    {
        "text": "so forth but they're kind of like toy testing\nthings. We're in a research phase and the",
        "start": 5154.68,
        "duration": 4.09
    },
    {
        "text": "kind of thing that they're talking about is\nfar away but at the same time I think we should",
        "start": 5158.77,
        "duration": 3.62
    },
    {
        "text": "be thinking about these questions now because\neventually we are going to have robots that",
        "start": 5162.39,
        "duration": 4.05
    },
    {
        "text": "are as intelligent as people and we want to\nmake sure that they do the right thing.",
        "start": 5166.44,
        "duration": 4.25
    },
    {
        "text": "So eventually you listen to climate scientists\nand we don't have quite that much time left",
        "start": 5170.69,
        "duration": 4.83
    },
    {
        "text": "to figure things out at all. But it depends\non how it how it goes.",
        "start": 5175.52,
        "duration": 4.15
    },
    {
        "text": "But you think you\nWe should be researching both and taking both",
        "start": 5179.67,
        "duration": 2.93
    },
    {
        "text": "seriously.\nAll right. Fernando.",
        "start": 5182.6,
        "duration": 1.59
    },
    {
        "text": "I mean. I think the problem exists right now.\nI mean I think there is a lot of intelligence",
        "start": 5184.19,
        "duration": 4.4
    },
    {
        "text": "and may not be completely autonomous general\nintelligence but there is a lot of the same",
        "start": 5188.59,
        "duration": 4.879
    },
    {
        "text": "techniques that are being used for AI in your\nsystems right now in your phones et cetera",
        "start": 5193.469,
        "duration": 5.391
    },
    {
        "text": "and they are susceptible to harming users\nmaybe not killing them but maybe reducing",
        "start": 5198.86,
        "duration": 8.1
    },
    {
        "text": "their quality of life or having disparate\nimpact on different populations of users.",
        "start": 5206.96,
        "duration": 4.92
    },
    {
        "text": "And that is a real effect right now and I\nthink we do need to start looking at that.",
        "start": 5211.88,
        "duration": 4.12
    },
    {
        "text": "So\nyou're not quite as alarmed as Musk and Hawking",
        "start": 5216.0,
        "duration": 2.521
    },
    {
        "text": "sound but...\nthere are things to worry about. Now",
        "start": 5218.521,
        "duration": 3.839
    },
    {
        "text": "I think we're agreed on that. We were not\nworried about the Terminator scenarios where",
        "start": 5222.36,
        "duration": 4.79
    },
    {
        "text": "the robots try and take over if they you know\nthey have trouble with tables they're not",
        "start": 5227.15,
        "duration": 3.71
    },
    {
        "text": "going to take over. They are already you know\nit doesn't forget about the physical robots",
        "start": 5230.86,
        "duration": 4.93
    },
    {
        "text": "the software bots that make decisions about\nmortgages are already an issue that we have",
        "start": 5235.79,
        "duration": 3.96
    },
    {
        "text": "to care about.\nAnd the killer drone in the atmosphere doesn't",
        "start": 5239.75,
        "duration": 2.23
    },
    {
        "text": "have to worry about a table. It just has a\nvery narrow purpose. And I'm remembering that",
        "start": 5241.98,
        "duration": 5.0
    },
    {
        "text": "Commander Data eventually had an evil twin\nbecause having been invented by humans humans",
        "start": 5246.98,
        "duration": 5.83
    },
    {
        "text": "couldn't resist somehow. What do you think\nabout Musk and Hawking's concern or you with",
        "start": 5252.81,
        "duration": 4.57
    },
    {
        "text": "them and how alarmed they are?\nThey're, there are some people who discuss",
        "start": 5257.38,
        "duration": 2.93
    },
    {
        "text": "superintelligence right now. All\nright. They have all sorts of stories to tell",
        "start": 5260.31,
        "duration": 3.84
    },
    {
        "text": "about what that looks like even though we\nhave not the slightest clue of what that could",
        "start": 5264.15,
        "duration": 4.33
    },
    {
        "text": "be. I don't think there's a worry about that.\nBut I do think there's a worry about existing",
        "start": 5268.48,
        "duration": 5.96
    },
    {
        "text": "systems already. Right. As you've seen in\nthe dilemmas before. If you look at the autonomously",
        "start": 5274.44,
        "duration": 5.6
    },
    {
        "text": "driving cars, people face a variety of dilemma\nlike situations where they have to make a",
        "start": 5280.04,
        "duration": 4.45
    },
    {
        "text": "judgment call do a break or do I not break,\ndo I swerve. Do I break and run over a child",
        "start": 5284.49,
        "duration": 5.28
    },
    {
        "text": "or do I swerve off and risk the life of the\ndriver and crash into a wall. These are the",
        "start": 5289.77,
        "duration": 4.15
    },
    {
        "text": "kinds of situations we have to address right\nnow. And as the systems are getting more autonomous",
        "start": 5293.92,
        "duration": 4.92
    },
    {
        "text": "we have to really speed that process up.\nYou're not quite as alarmed as they are but",
        "start": 5298.84,
        "duration": 3.79
    },
    {
        "text": "you're saying we've got to stay of it. No\nI'm alarmed for a different reason because",
        "start": 5302.63,
        "duration": 3.16
    },
    {
        "text": "these systems already exist. Right. We're\nworking on those systems. We don't need the",
        "start": 5305.79,
        "duration": 4.33
    },
    {
        "text": "super intelligence or the human level intelligence\nAI in order to face exactly some of those",
        "start": 5310.12,
        "duration": 5.56
    },
    {
        "text": "questions. Right.\nWendell",
        "start": 5315.68,
        "duration": 1.0
    },
    {
        "text": "I think we perhaps misunderstood their alarm\nbell. Not a one of them is saying we should",
        "start": 5316.68,
        "duration": 6.32
    },
    {
        "text": "stop developing AI and I think if they really\nbelieve",
        "start": 5323.0,
        "duration": 2.62
    },
    {
        "text": "Summoning the demon though, those strong words.\nBut let me finish. That's not one of them",
        "start": 5325.62,
        "duration": 5.34
    },
    {
        "text": "has said we should stop building AI. And furthermore\nElon Musk is spending more money on the development",
        "start": 5330.96,
        "duration": 6.21
    },
    {
        "text": "of artificial intelligence than anyone. So\nwhat has really occurred through these warnings",
        "start": 5337.17,
        "duration": 6.25
    },
    {
        "text": "is to just ask that we now begin to direct\ntheir attention to be sure that in the development",
        "start": 5343.42,
        "duration": 4.98
    },
    {
        "text": "of artificial intelligence we can ensure that\nit's beneficial, that it's robust but it's",
        "start": 5348.4,
        "duration": 5.71
    },
    {
        "text": "safe and it is controllable. And I actually\nthink that's been achieved. We're sitting",
        "start": 5354.11,
        "duration": 4.53
    },
    {
        "text": "here talking about this today. I've been in\nthis field of machine ethics and robot ethics",
        "start": 5358.64,
        "duration": 5.92
    },
    {
        "text": "for about 13 years now and I would say in\nthe last year and a half there's been more",
        "start": 5364.56,
        "duration": 4.63
    },
    {
        "text": "attention to this subject than there have\nbeen for the previous 12.",
        "start": 5369.19,
        "duration": 4.14
    },
    {
        "text": "Thanks. Linell, you're you're a mother you\nsaid you have a daughter who is in the house",
        "start": 5373.33,
        "duration": 7.08
    },
    {
        "text": "and is now going to be facing a very different\nworld than you were born into.",
        "start": 5380.41,
        "duration": 4.0
    },
    {
        "text": "Oh absolutely and she explains what SIRI is\nall the time to me but I just like to continue",
        "start": 5384.41,
        "duration": 5.059
    },
    {
        "text": "with what Wendell indicated is that alarmed.\nNo.",
        "start": 5389.469,
        "duration": 5.071
    },
    {
        "text": "but it does stress the need to have more conversations\nlike we're having today. Not with engineers",
        "start": 5394.54,
        "duration": 4.83
    },
    {
        "text": "talking to engineers and attorneys only talking\nto other attorneys but instead to talk across",
        "start": 5399.37,
        "duration": 5.49
    },
    {
        "text": "fields. To ensure that we're speaking the\nsame language because we cannot figure out",
        "start": 5404.86,
        "duration": 5.839
    },
    {
        "text": "these issues in our own individual cylinders\nof excellence otherwise known as stovepipes.",
        "start": 5410.699,
        "duration": 6.931
    },
    {
        "text": "Instead we have to talk across fields and\nbe able to address these issues anticipate",
        "start": 5417.63,
        "duration": 4.52
    },
    {
        "text": "what the future problems are going to be.\nAnd if we do that and we start talking it",
        "start": 5422.15,
        "duration": 5.08
    },
    {
        "text": "using similar definitions and and start talking\nthe same language autonomous systems I think",
        "start": 5427.23,
        "duration": 5.719
    },
    {
        "text": "are going to mirror what Gary talks about.\nThe more positive aspects, regardless of what",
        "start": 5432.949,
        "duration": 5.382
    },
    {
        "text": "field we're talking about and much less the\nnegative ones.",
        "start": 5438.331,
        "duration": 2.719
    },
    {
        "text": "Connecting the stovepipes and very definitely\nimportant, law.",
        "start": 5441.05,
        "duration": 3.41
    },
    {
        "text": "We shouldn't be afraid of seeing new kinds\nof languages and all kinds of fields because",
        "start": 5444.46,
        "duration": 4.11
    },
    {
        "text": "they have to talk to each other so that we\ndon't have other problems that we've had because",
        "start": 5448.57,
        "duration": 3.26
    },
    {
        "text": "the stovepipes weren't connected before.\nAbsolutely. I mean you have to teach attorneys",
        "start": 5451.83,
        "duration": 3.869
    },
    {
        "text": "a few things about control systems and feedback\nloops though.",
        "start": 5455.699,
        "duration": 2.631
    },
    {
        "text": "But he doesn't have to learn code does he.\nNo but then again they have to be able to",
        "start": 5458.33,
        "duration": 5.5
    },
    {
        "text": "have a conversation with an engineer and not\nhave the eyes glaze over.",
        "start": 5463.83,
        "duration": 4.75
    },
    {
        "text": "Well when lawyers get to talking law they're\ntalking their own code after all.",
        "start": 5468.58,
        "duration": 3.07
    },
    {
        "text": "That's right. We have to stop using Latin.\nI like latin. Some of it. Some of it. Well",
        "start": 5471.65,
        "duration": 7.22
    },
    {
        "text": "listen tomorrow at 1:30 at NYU there's going\nto be a salon on some aspects of this. Paging",
        "start": 5478.87,
        "duration": 7.84
    },
    {
        "text": "Dr. robot how AI is revolutionizing health\ncare. You can find that on line I think exactly",
        "start": 5486.71,
        "duration": 6.009
    },
    {
        "text": "how to buy a ticket for that at 1:30 tomorrow\nat NYU. How AI is revolution revolutionizing",
        "start": 5492.719,
        "duration": 5.0
    },
    {
        "text": "health care. Otherwise we're done. Our participants\nwill be in the lobby. Let's give them a round",
        "start": 5497.719,
        "duration": 44.431
    },
    {
        "text": "of applause.",
        "start": 5542.15,
        "duration": 79.339
    }
]